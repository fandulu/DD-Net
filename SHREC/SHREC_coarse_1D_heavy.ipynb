{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import glob\n",
    "import gc\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers.convolutional import *\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "\n",
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.frame_l = 32 # the length of frames\n",
    "        self.joint_n = 12 # the number of joints\n",
    "        self.joint_n = 22 # the number of joints\n",
    "        self.joint_d = 3 # the dimension of joints\n",
    "        self.clc_coarse = 14 # the number of coarse class\n",
    "        self.clc_fine = 28 # the number of fine-grained class\n",
    "        self.feat_d = 231\n",
    "        self.filters = 64\n",
    "        self.data_dir = '/mnt/nasbi/homes/fan/projects/action/skeleton/data/SHREC/'\n",
    "C = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poses_diff(x):\n",
    "    H, W = x.get_shape()[1],x.get_shape()[2]\n",
    "    x = tf.subtract(x[:,1:,...],x[:,:-1,...])\n",
    "    x = tf.image.resize_nearest_neighbor(x,size=[H.value,W.value],align_corners=False) # should not alignment here\n",
    "    return x\n",
    "\n",
    "def pose_motion(P,frame_l):\n",
    "    P_diff_slow = Lambda(lambda x: poses_diff(x))(P)\n",
    "    P_diff_slow = Reshape((frame_l,-1))(P_diff_slow)\n",
    "    P_fast = Lambda(lambda x: x[:,::2,...])(P)\n",
    "    P_diff_fast = Lambda(lambda x: poses_diff(x))(P_fast)\n",
    "    P_diff_fast = Reshape((int(frame_l/2),-1))(P_diff_fast)\n",
    "    return P_diff_slow,P_diff_fast\n",
    "    \n",
    "def c1D(x,filters,kernel):\n",
    "    x = Conv1D(filters, kernel_size=kernel,padding='same',use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def block(x,filters):\n",
    "    x = c1D(x,filters,3)\n",
    "    x = c1D(x,filters,3)\n",
    "    return x\n",
    "    \n",
    "def d1D(x,filters):\n",
    "    x = Dense(filters,use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def build_FM(frame_l=32,joint_n=22,joint_d=2,feat_d=231,filters=16):   \n",
    "    M = Input(shape=(frame_l,feat_d))\n",
    "    P = Input(shape=(frame_l,joint_n,joint_d))\n",
    "    \n",
    "    diff_slow,diff_fast = pose_motion(P,frame_l)\n",
    "    \n",
    "    x = c1D(M,filters*2,1)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = c1D(x,filters,3)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = c1D(x,filters,1)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "\n",
    "    x_d_slow = c1D(diff_slow,filters*2,1)\n",
    "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
    "    x_d_slow = c1D(x_d_slow,filters,3)\n",
    "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
    "    x_d_slow = c1D(x_d_slow,filters,1)\n",
    "    x_d_slow = MaxPool1D(2)(x_d_slow)\n",
    "    x_d_slow = SpatialDropout1D(0.1)(x_d_slow)\n",
    "        \n",
    "    x_d_fast = c1D(diff_fast,filters*2,1)\n",
    "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
    "    x_d_fast = c1D(x_d_fast,filters,3) \n",
    "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
    "    x_d_fast = c1D(x_d_fast,filters,1) \n",
    "    x_d_fast = SpatialDropout1D(0.1)(x_d_fast)\n",
    "   \n",
    "    x = concatenate([x,x_d_slow,x_d_fast])\n",
    "    x = block(x,filters*2)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = block(x,filters*4)\n",
    "    x = MaxPool1D(2)(x)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "\n",
    "    x = block(x,filters*8)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    return Model(inputs=[M,P],outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_DD_Net(frame_l=32,joint_n=22,joint_d=3,feat_d=231,clc_num=14,filters=16):\n",
    "    M = Input(name='M', shape=(frame_l,feat_d))  \n",
    "    P = Input(name='P', shape=(frame_l,joint_n,joint_d)) \n",
    "    \n",
    "    FM = build_FM(frame_l,joint_n,joint_d,feat_d,filters)\n",
    "    \n",
    "    x = FM([M,P])\n",
    "\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    \n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = d1D(x,128)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(clc_num, activation='softmax')(x)\n",
    "    \n",
    "    ######################Self-supervised part\n",
    "    model = Model(inputs=[M,P],outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DD_Net = build_DD_Net(C.frame_l,C.joint_n,C.joint_d,C.feat_d,C.clc_coarse,C.filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "M (InputLayer)                  (None, 32, 231)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "P (InputLayer)                  (None, 32, 22, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 (None, 4, 512)       1740160     M[0][0]                          \n",
      "                                                                 P[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 512)          0           model_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          65536       global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 128)          512         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)      (None, 128)          0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           leaky_re_lu_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          16384       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 128)          512         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)      (None, 128)          0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           leaky_re_lu_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 14)           1806        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,824,910\n",
      "Trainable params: 1,819,278\n",
      "Non-trainable params: 5,632\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DD_Net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train = pickle.load(open(C.data_dir+\"train.pkl\", \"rb\"))\n",
    "Test = pickle.load(open(C.data_dir+\"test.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without frame_sampling train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = []\n",
    "X_1 = []\n",
    "Y = []\n",
    "for i in tqdm(range(len(Train['pose']))): \n",
    "    p = np.copy(Train['pose'][i]).reshape([-1,22,3])\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_coarse)\n",
    "    label[Train['coarse_label'][i]-1] = 1   \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_0.append(M)\n",
    "    X_1.append(p)\n",
    "    Y.append(label)\n",
    "\n",
    "X_0 = np.stack(X_0)  \n",
    "X_1 = np.stack(X_1) \n",
    "Y = np.stack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_0 = []\n",
    "X_test_1 = []\n",
    "Y_test = []\n",
    "for i in tqdm(range(len(Test['pose']))): \n",
    "    p = np.copy(Test['pose'][i]).reshape([-1,22,3])\n",
    "    p = zoom(p,target_l=C.frame_l,joints_num=C.joint_n,joints_dim=C.joint_d)\n",
    "    p = normlize_range(p)\n",
    "    \n",
    "    label = np.zeros(C.clc_coarse)\n",
    "    label[Test['coarse_label'][i]-1] = 1   \n",
    "\n",
    "    M = get_CG(p,C)\n",
    "\n",
    "    X_test_0.append(M)\n",
    "    X_test_1.append(p)\n",
    "    Y_test.append(label)\n",
    "\n",
    "X_test_0 = np.stack(X_test_0) \n",
    "X_test_1 = np.stack(X_test_1)  \n",
    "Y_test = np.stack(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1960 samples, validate on 840 samples\n",
      "Epoch 1/400\n",
      "1960/1960 [==============================] - 10s 5ms/step - loss: 3.4202 - acc: 0.0699 - val_loss: 2.3399 - val_acc: 0.2333\n",
      "Epoch 2/400\n",
      "1960/1960 [==============================] - 1s 278us/step - loss: 2.9979 - acc: 0.1087 - val_loss: 2.0753 - val_acc: 0.3714\n",
      "Epoch 3/400\n",
      "1960/1960 [==============================] - 1s 332us/step - loss: 2.7686 - acc: 0.1429 - val_loss: 1.9081 - val_acc: 0.4595\n",
      "Epoch 4/400\n",
      "1960/1960 [==============================] - 1s 283us/step - loss: 2.5301 - acc: 0.1980 - val_loss: 1.8058 - val_acc: 0.4702\n",
      "Epoch 5/400\n",
      "1960/1960 [==============================] - 1s 359us/step - loss: 2.3672 - acc: 0.2459 - val_loss: 1.7739 - val_acc: 0.4631\n",
      "Epoch 6/400\n",
      "1960/1960 [==============================] - 0s 250us/step - loss: 2.1765 - acc: 0.2949 - val_loss: 1.6809 - val_acc: 0.4786\n",
      "Epoch 7/400\n",
      "1960/1960 [==============================] - 1s 360us/step - loss: 2.0163 - acc: 0.3485 - val_loss: 1.5215 - val_acc: 0.5155\n",
      "Epoch 8/400\n",
      "1960/1960 [==============================] - 1s 261us/step - loss: 1.8562 - acc: 0.4051 - val_loss: 1.3505 - val_acc: 0.5798\n",
      "Epoch 9/400\n",
      "1960/1960 [==============================] - 1s 323us/step - loss: 1.7096 - acc: 0.4515 - val_loss: 1.2508 - val_acc: 0.6107\n",
      "Epoch 10/400\n",
      "1960/1960 [==============================] - 1s 276us/step - loss: 1.5808 - acc: 0.4995 - val_loss: 1.1880 - val_acc: 0.6381\n",
      "Epoch 11/400\n",
      "1960/1960 [==============================] - 1s 332us/step - loss: 1.4497 - acc: 0.5449 - val_loss: 1.1243 - val_acc: 0.6655\n",
      "Epoch 12/400\n",
      "1960/1960 [==============================] - 1s 256us/step - loss: 1.3196 - acc: 0.5980 - val_loss: 1.0547 - val_acc: 0.6988\n",
      "Epoch 13/400\n",
      "1960/1960 [==============================] - 1s 318us/step - loss: 1.2258 - acc: 0.6429 - val_loss: 0.9960 - val_acc: 0.7238\n",
      "Epoch 14/400\n",
      "1960/1960 [==============================] - 0s 255us/step - loss: 1.1471 - acc: 0.6592 - val_loss: 0.9373 - val_acc: 0.7476\n",
      "Epoch 15/400\n",
      "1960/1960 [==============================] - 1s 355us/step - loss: 1.0561 - acc: 0.7026 - val_loss: 0.8892 - val_acc: 0.7583\n",
      "Epoch 16/400\n",
      "1960/1960 [==============================] - 1s 259us/step - loss: 1.0072 - acc: 0.7112 - val_loss: 0.8471 - val_acc: 0.7702\n",
      "Epoch 17/400\n",
      "1960/1960 [==============================] - 1s 437us/step - loss: 0.9552 - acc: 0.7245 - val_loss: 0.8132 - val_acc: 0.7726\n",
      "Epoch 18/400\n",
      "1960/1960 [==============================] - 0s 248us/step - loss: 0.8831 - acc: 0.7679 - val_loss: 0.7866 - val_acc: 0.7821\n",
      "Epoch 19/400\n",
      "1960/1960 [==============================] - 1s 356us/step - loss: 0.8308 - acc: 0.7714 - val_loss: 0.7603 - val_acc: 0.7893\n",
      "Epoch 20/400\n",
      "1960/1960 [==============================] - 1s 257us/step - loss: 0.7895 - acc: 0.7908 - val_loss: 0.7378 - val_acc: 0.7964\n",
      "Epoch 21/400\n",
      "1960/1960 [==============================] - 1s 329us/step - loss: 0.7333 - acc: 0.8031 - val_loss: 0.7140 - val_acc: 0.8048\n",
      "Epoch 22/400\n",
      "1960/1960 [==============================] - 1s 257us/step - loss: 0.6975 - acc: 0.8281 - val_loss: 0.6882 - val_acc: 0.8131\n",
      "Epoch 23/400\n",
      "1960/1960 [==============================] - 1s 366us/step - loss: 0.6597 - acc: 0.8383 - val_loss: 0.6655 - val_acc: 0.8202\n",
      "Epoch 24/400\n",
      "1960/1960 [==============================] - 0s 247us/step - loss: 0.6333 - acc: 0.8474 - val_loss: 0.6414 - val_acc: 0.8298\n",
      "Epoch 25/400\n",
      "1960/1960 [==============================] - 1s 348us/step - loss: 0.5943 - acc: 0.8515 - val_loss: 0.6242 - val_acc: 0.8381\n",
      "Epoch 26/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 0.5482 - acc: 0.8643 - val_loss: 0.6124 - val_acc: 0.8369\n",
      "Epoch 27/400\n",
      "1960/1960 [==============================] - 1s 346us/step - loss: 0.5511 - acc: 0.8638 - val_loss: 0.5977 - val_acc: 0.8429\n",
      "Epoch 28/400\n",
      "1960/1960 [==============================] - 1s 260us/step - loss: 0.5005 - acc: 0.8786 - val_loss: 0.5803 - val_acc: 0.8440\n",
      "Epoch 29/400\n",
      "1960/1960 [==============================] - 1s 337us/step - loss: 0.4856 - acc: 0.8903 - val_loss: 0.5590 - val_acc: 0.8500\n",
      "Epoch 30/400\n",
      "1960/1960 [==============================] - 0s 248us/step - loss: 0.4569 - acc: 0.8893 - val_loss: 0.5303 - val_acc: 0.8607\n",
      "Epoch 31/400\n",
      "1960/1960 [==============================] - 1s 329us/step - loss: 0.4334 - acc: 0.9077 - val_loss: 0.5023 - val_acc: 0.8667\n",
      "Epoch 32/400\n",
      "1960/1960 [==============================] - 1s 268us/step - loss: 0.4168 - acc: 0.9056 - val_loss: 0.4737 - val_acc: 0.8738\n",
      "Epoch 33/400\n",
      "1960/1960 [==============================] - 1s 367us/step - loss: 0.4053 - acc: 0.9173 - val_loss: 0.4499 - val_acc: 0.8774\n",
      "Epoch 34/400\n",
      "1960/1960 [==============================] - 1s 296us/step - loss: 0.3783 - acc: 0.9189 - val_loss: 0.4301 - val_acc: 0.8857\n",
      "Epoch 35/400\n",
      "1960/1960 [==============================] - 1s 315us/step - loss: 0.3651 - acc: 0.9250 - val_loss: 0.4147 - val_acc: 0.8881\n",
      "Epoch 36/400\n",
      "1960/1960 [==============================] - 1s 260us/step - loss: 0.3568 - acc: 0.9260 - val_loss: 0.4049 - val_acc: 0.8893\n",
      "Epoch 37/400\n",
      "1960/1960 [==============================] - 1s 343us/step - loss: 0.3365 - acc: 0.9276 - val_loss: 0.3984 - val_acc: 0.8917\n",
      "Epoch 38/400\n",
      "1960/1960 [==============================] - 1s 255us/step - loss: 0.3380 - acc: 0.9199 - val_loss: 0.3932 - val_acc: 0.8917\n",
      "Epoch 39/400\n",
      "1960/1960 [==============================] - 1s 327us/step - loss: 0.2961 - acc: 0.9393 - val_loss: 0.3895 - val_acc: 0.8905\n",
      "Epoch 40/400\n",
      "1960/1960 [==============================] - 0s 255us/step - loss: 0.2954 - acc: 0.9388 - val_loss: 0.3863 - val_acc: 0.8964\n",
      "Epoch 41/400\n",
      "1960/1960 [==============================] - 1s 338us/step - loss: 0.2761 - acc: 0.9449 - val_loss: 0.3836 - val_acc: 0.8976\n",
      "Epoch 42/400\n",
      "1960/1960 [==============================] - 1s 274us/step - loss: 0.2640 - acc: 0.9500 - val_loss: 0.3811 - val_acc: 0.9024\n",
      "Epoch 43/400\n",
      "1960/1960 [==============================] - 1s 331us/step - loss: 0.2721 - acc: 0.9444 - val_loss: 0.3759 - val_acc: 0.9036\n",
      "Epoch 44/400\n",
      "1960/1960 [==============================] - 1s 273us/step - loss: 0.2676 - acc: 0.9495 - val_loss: 0.3707 - val_acc: 0.9071\n",
      "Epoch 45/400\n",
      "1960/1960 [==============================] - 1s 324us/step - loss: 0.2546 - acc: 0.9510 - val_loss: 0.3679 - val_acc: 0.9060\n",
      "Epoch 46/400\n",
      "1960/1960 [==============================] - 0s 249us/step - loss: 0.2356 - acc: 0.9597 - val_loss: 0.3625 - val_acc: 0.9060\n",
      "Epoch 47/400\n",
      "1960/1960 [==============================] - 1s 325us/step - loss: 0.2191 - acc: 0.9622 - val_loss: 0.3569 - val_acc: 0.9071\n",
      "Epoch 48/400\n",
      "1960/1960 [==============================] - 1s 271us/step - loss: 0.2203 - acc: 0.9592 - val_loss: 0.3550 - val_acc: 0.9036\n",
      "Epoch 49/400\n",
      "1960/1960 [==============================] - 1s 342us/step - loss: 0.2096 - acc: 0.9612 - val_loss: 0.3551 - val_acc: 0.9024\n",
      "Epoch 50/400\n",
      "1960/1960 [==============================] - 1s 260us/step - loss: 0.2084 - acc: 0.9622 - val_loss: 0.3527 - val_acc: 0.8988\n",
      "Epoch 51/400\n",
      "1960/1960 [==============================] - 1s 333us/step - loss: 0.1905 - acc: 0.9633 - val_loss: 0.3471 - val_acc: 0.9036\n",
      "Epoch 52/400\n",
      "1960/1960 [==============================] - 1s 261us/step - loss: 0.2003 - acc: 0.9648 - val_loss: 0.3438 - val_acc: 0.9048\n",
      "Epoch 53/400\n",
      "1960/1960 [==============================] - 1s 364us/step - loss: 0.1775 - acc: 0.9689 - val_loss: 0.3378 - val_acc: 0.9083\n",
      "Epoch 54/400\n",
      "1960/1960 [==============================] - 1s 264us/step - loss: 0.1833 - acc: 0.9704 - val_loss: 0.3329 - val_acc: 0.9095\n",
      "Epoch 55/400\n",
      "1960/1960 [==============================] - 1s 349us/step - loss: 0.1764 - acc: 0.9699 - val_loss: 0.3303 - val_acc: 0.9119\n",
      "Epoch 56/400\n",
      "1960/1960 [==============================] - 1s 262us/step - loss: 0.1750 - acc: 0.9673 - val_loss: 0.3288 - val_acc: 0.9131\n",
      "Epoch 57/400\n",
      "1960/1960 [==============================] - 1s 359us/step - loss: 0.1612 - acc: 0.9770 - val_loss: 0.3228 - val_acc: 0.9131\n",
      "Epoch 58/400\n",
      "1960/1960 [==============================] - 1s 279us/step - loss: 0.1486 - acc: 0.9750 - val_loss: 0.3176 - val_acc: 0.9179\n",
      "Epoch 59/400\n",
      "1960/1960 [==============================] - 1s 359us/step - loss: 0.1579 - acc: 0.9730 - val_loss: 0.3110 - val_acc: 0.9190\n",
      "Epoch 60/400\n",
      "1960/1960 [==============================] - 1s 288us/step - loss: 0.1435 - acc: 0.9791 - val_loss: 0.3091 - val_acc: 0.9155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/400\n",
      "1960/1960 [==============================] - 1s 354us/step - loss: 0.1521 - acc: 0.9760 - val_loss: 0.3010 - val_acc: 0.9143\n",
      "Epoch 62/400\n",
      "1960/1960 [==============================] - 1s 276us/step - loss: 0.1346 - acc: 0.9811 - val_loss: 0.2938 - val_acc: 0.9179\n",
      "Epoch 63/400\n",
      "1960/1960 [==============================] - 1s 383us/step - loss: 0.1352 - acc: 0.9801 - val_loss: 0.2852 - val_acc: 0.9202\n",
      "Epoch 64/400\n",
      "1960/1960 [==============================] - 1s 288us/step - loss: 0.1208 - acc: 0.9811 - val_loss: 0.2799 - val_acc: 0.9226\n",
      "Epoch 65/400\n",
      "1960/1960 [==============================] - 1s 347us/step - loss: 0.1283 - acc: 0.9796 - val_loss: 0.2771 - val_acc: 0.9250\n",
      "Epoch 66/400\n",
      "1960/1960 [==============================] - 1s 273us/step - loss: 0.1172 - acc: 0.9857 - val_loss: 0.2779 - val_acc: 0.9238\n",
      "Epoch 67/400\n",
      "1960/1960 [==============================] - 1s 327us/step - loss: 0.1260 - acc: 0.9816 - val_loss: 0.2783 - val_acc: 0.9238\n",
      "Epoch 68/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 0.1249 - acc: 0.9811 - val_loss: 0.2791 - val_acc: 0.9226\n",
      "Epoch 69/400\n",
      "1960/1960 [==============================] - 1s 361us/step - loss: 0.1123 - acc: 0.9842 - val_loss: 0.2803 - val_acc: 0.9238\n",
      "Epoch 70/400\n",
      "1960/1960 [==============================] - 1s 258us/step - loss: 0.1094 - acc: 0.9883 - val_loss: 0.2841 - val_acc: 0.9262\n",
      "Epoch 71/400\n",
      "1960/1960 [==============================] - 1s 337us/step - loss: 0.1104 - acc: 0.9811 - val_loss: 0.2869 - val_acc: 0.9262\n",
      "Epoch 72/400\n",
      "1960/1960 [==============================] - 1s 275us/step - loss: 0.1051 - acc: 0.9837 - val_loss: 0.2905 - val_acc: 0.9274\n",
      "Epoch 73/400\n",
      "1960/1960 [==============================] - 1s 354us/step - loss: 0.1008 - acc: 0.9862 - val_loss: 0.2929 - val_acc: 0.9226\n",
      "Epoch 74/400\n",
      "1960/1960 [==============================] - 0s 252us/step - loss: 0.1010 - acc: 0.9888 - val_loss: 0.2977 - val_acc: 0.9214\n",
      "Epoch 75/400\n",
      "1960/1960 [==============================] - 1s 331us/step - loss: 0.0896 - acc: 0.9867 - val_loss: 0.2982 - val_acc: 0.9214\n",
      "Epoch 76/400\n",
      "1960/1960 [==============================] - 1s 259us/step - loss: 0.0934 - acc: 0.9867 - val_loss: 0.2962 - val_acc: 0.9214\n",
      "Epoch 77/400\n",
      "1960/1960 [==============================] - 1s 335us/step - loss: 0.0896 - acc: 0.9872 - val_loss: 0.2956 - val_acc: 0.9238\n",
      "Epoch 78/400\n",
      "1960/1960 [==============================] - 1s 262us/step - loss: 0.0895 - acc: 0.9878 - val_loss: 0.2905 - val_acc: 0.9286\n",
      "Epoch 79/400\n",
      "1960/1960 [==============================] - 1s 345us/step - loss: 0.0812 - acc: 0.9898 - val_loss: 0.2855 - val_acc: 0.9333\n",
      "Epoch 80/400\n",
      "1960/1960 [==============================] - 0s 250us/step - loss: 0.0854 - acc: 0.9852 - val_loss: 0.2790 - val_acc: 0.9333\n",
      "Epoch 81/400\n",
      "1960/1960 [==============================] - 1s 372us/step - loss: 0.0879 - acc: 0.9867 - val_loss: 0.2719 - val_acc: 0.9333\n",
      "Epoch 82/400\n",
      "1960/1960 [==============================] - 1s 257us/step - loss: 0.0820 - acc: 0.9867 - val_loss: 0.2675 - val_acc: 0.9333\n",
      "Epoch 83/400\n",
      "1960/1960 [==============================] - 1s 337us/step - loss: 0.0827 - acc: 0.9888 - val_loss: 0.2642 - val_acc: 0.9321\n",
      "Epoch 84/400\n",
      "1960/1960 [==============================] - 1s 261us/step - loss: 0.0818 - acc: 0.9878 - val_loss: 0.2655 - val_acc: 0.9333\n",
      "Epoch 85/400\n",
      "1960/1960 [==============================] - 1s 350us/step - loss: 0.0779 - acc: 0.9923 - val_loss: 0.2665 - val_acc: 0.9298\n",
      "Epoch 86/400\n",
      "1960/1960 [==============================] - 1s 288us/step - loss: 0.0766 - acc: 0.9913 - val_loss: 0.2671 - val_acc: 0.9262\n",
      "Epoch 87/400\n",
      "1960/1960 [==============================] - 1s 309us/step - loss: 0.0798 - acc: 0.9893 - val_loss: 0.2658 - val_acc: 0.9298\n",
      "Epoch 88/400\n",
      "1960/1960 [==============================] - 1s 281us/step - loss: 0.0766 - acc: 0.9893 - val_loss: 0.2629 - val_acc: 0.9298\n",
      "Epoch 89/400\n",
      "1960/1960 [==============================] - 1s 310us/step - loss: 0.0696 - acc: 0.9908 - val_loss: 0.2582 - val_acc: 0.9321\n",
      "Epoch 90/400\n",
      "1960/1960 [==============================] - 1s 259us/step - loss: 0.0706 - acc: 0.9893 - val_loss: 0.2544 - val_acc: 0.9333\n",
      "Epoch 91/400\n",
      "1960/1960 [==============================] - 1s 315us/step - loss: 0.0747 - acc: 0.9913 - val_loss: 0.2513 - val_acc: 0.9321\n",
      "Epoch 92/400\n",
      "1960/1960 [==============================] - 0s 254us/step - loss: 0.0617 - acc: 0.9934 - val_loss: 0.2501 - val_acc: 0.9298\n",
      "Epoch 93/400\n",
      "1960/1960 [==============================] - 1s 319us/step - loss: 0.0642 - acc: 0.9939 - val_loss: 0.2510 - val_acc: 0.9310\n",
      "Epoch 94/400\n",
      "1960/1960 [==============================] - 0s 240us/step - loss: 0.0710 - acc: 0.9898 - val_loss: 0.2494 - val_acc: 0.9298\n",
      "Epoch 95/400\n",
      "1960/1960 [==============================] - 1s 314us/step - loss: 0.0689 - acc: 0.9923 - val_loss: 0.2464 - val_acc: 0.9321\n",
      "Epoch 96/400\n",
      "1960/1960 [==============================] - 0s 249us/step - loss: 0.0650 - acc: 0.9929 - val_loss: 0.2439 - val_acc: 0.9333\n",
      "Epoch 97/400\n",
      "1960/1960 [==============================] - 1s 322us/step - loss: 0.0625 - acc: 0.9929 - val_loss: 0.2417 - val_acc: 0.9333\n",
      "Epoch 98/400\n",
      "1960/1960 [==============================] - 0s 249us/step - loss: 0.0625 - acc: 0.9934 - val_loss: 0.2403 - val_acc: 0.9357\n",
      "Epoch 99/400\n",
      "1960/1960 [==============================] - 1s 326us/step - loss: 0.0664 - acc: 0.9918 - val_loss: 0.2394 - val_acc: 0.9357\n",
      "Epoch 100/400\n",
      "1960/1960 [==============================] - 1s 259us/step - loss: 0.0680 - acc: 0.9923 - val_loss: 0.2386 - val_acc: 0.9369\n",
      "Epoch 101/400\n",
      "1960/1960 [==============================] - 1s 352us/step - loss: 0.0618 - acc: 0.9929 - val_loss: 0.2381 - val_acc: 0.9381\n",
      "Epoch 102/400\n",
      "1960/1960 [==============================] - 1s 265us/step - loss: 0.0650 - acc: 0.9918 - val_loss: 0.2379 - val_acc: 0.9381\n",
      "Epoch 103/400\n",
      "1960/1960 [==============================] - 1s 329us/step - loss: 0.0604 - acc: 0.9949 - val_loss: 0.2378 - val_acc: 0.9405\n",
      "Epoch 104/400\n",
      "1960/1960 [==============================] - 1s 269us/step - loss: 0.0608 - acc: 0.9939 - val_loss: 0.2377 - val_acc: 0.9417\n",
      "Epoch 105/400\n",
      "1960/1960 [==============================] - 1s 302us/step - loss: 0.0615 - acc: 0.9929 - val_loss: 0.2378 - val_acc: 0.9417\n",
      "Epoch 106/400\n",
      "1960/1960 [==============================] - 1s 273us/step - loss: 0.0568 - acc: 0.9934 - val_loss: 0.2376 - val_acc: 0.9405\n",
      "Epoch 107/400\n",
      "1960/1960 [==============================] - 1s 286us/step - loss: 0.0607 - acc: 0.9939 - val_loss: 0.2371 - val_acc: 0.9417\n",
      "Epoch 108/400\n",
      "1960/1960 [==============================] - 0s 243us/step - loss: 0.0678 - acc: 0.9908 - val_loss: 0.2359 - val_acc: 0.9393\n",
      "Epoch 109/400\n",
      "1960/1960 [==============================] - 1s 295us/step - loss: 0.0626 - acc: 0.9908 - val_loss: 0.2344 - val_acc: 0.9405\n",
      "Epoch 110/400\n",
      "1960/1960 [==============================] - 0s 230us/step - loss: 0.0643 - acc: 0.9929 - val_loss: 0.2329 - val_acc: 0.9429\n",
      "Epoch 111/400\n",
      "1960/1960 [==============================] - 1s 305us/step - loss: 0.0597 - acc: 0.9929 - val_loss: 0.2319 - val_acc: 0.9429\n",
      "Epoch 112/400\n",
      "1960/1960 [==============================] - 0s 244us/step - loss: 0.0570 - acc: 0.9929 - val_loss: 0.2309 - val_acc: 0.9440\n",
      "Epoch 113/400\n",
      "1960/1960 [==============================] - 1s 325us/step - loss: 0.0629 - acc: 0.9913 - val_loss: 0.2300 - val_acc: 0.9440\n",
      "Epoch 114/400\n",
      "1960/1960 [==============================] - 1s 309us/step - loss: 0.0557 - acc: 0.9969 - val_loss: 0.2290 - val_acc: 0.9452\n",
      "Epoch 115/400\n",
      "1960/1960 [==============================] - 1s 424us/step - loss: 0.0540 - acc: 0.9939 - val_loss: 0.2277 - val_acc: 0.9452\n",
      "Epoch 116/400\n",
      "1960/1960 [==============================] - 1s 262us/step - loss: 0.0567 - acc: 0.9934 - val_loss: 0.2265 - val_acc: 0.9452\n",
      "Epoch 117/400\n",
      "1960/1960 [==============================] - 1s 304us/step - loss: 0.0578 - acc: 0.9923 - val_loss: 0.2253 - val_acc: 0.9452\n",
      "Epoch 118/400\n",
      "1960/1960 [==============================] - 1s 270us/step - loss: 0.0613 - acc: 0.9918 - val_loss: 0.2240 - val_acc: 0.9452\n",
      "Epoch 119/400\n",
      "1960/1960 [==============================] - 1s 310us/step - loss: 0.0621 - acc: 0.9918 - val_loss: 0.2229 - val_acc: 0.9452\n",
      "Epoch 120/400\n",
      "1960/1960 [==============================] - 1s 284us/step - loss: 0.0634 - acc: 0.9939 - val_loss: 0.2219 - val_acc: 0.9464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/400\n",
      "1960/1960 [==============================] - 1s 315us/step - loss: 0.0594 - acc: 0.9944 - val_loss: 0.2212 - val_acc: 0.9464\n",
      "Epoch 122/400\n",
      "1960/1960 [==============================] - 1s 258us/step - loss: 0.0567 - acc: 0.9949 - val_loss: 0.2205 - val_acc: 0.9464\n",
      "Epoch 123/400\n",
      "1960/1960 [==============================] - 1s 293us/step - loss: 0.0554 - acc: 0.9954 - val_loss: 0.2197 - val_acc: 0.9464\n",
      "Epoch 124/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 0.0578 - acc: 0.9923 - val_loss: 0.2191 - val_acc: 0.9464\n",
      "Epoch 125/400\n",
      "1960/1960 [==============================] - 1s 336us/step - loss: 0.0633 - acc: 0.9913 - val_loss: 0.2184 - val_acc: 0.9464\n",
      "Epoch 126/400\n",
      "1960/1960 [==============================] - 0s 238us/step - loss: 0.0579 - acc: 0.9929 - val_loss: 0.2176 - val_acc: 0.9464\n",
      "Epoch 127/400\n",
      "1960/1960 [==============================] - 1s 311us/step - loss: 0.0610 - acc: 0.9918 - val_loss: 0.2167 - val_acc: 0.9464\n",
      "Epoch 128/400\n",
      "1960/1960 [==============================] - 0s 248us/step - loss: 0.0578 - acc: 0.9944 - val_loss: 0.2158 - val_acc: 0.9464\n",
      "Epoch 129/400\n",
      "1960/1960 [==============================] - 1s 339us/step - loss: 0.0586 - acc: 0.9939 - val_loss: 0.2150 - val_acc: 0.9464\n",
      "Epoch 130/400\n",
      "1960/1960 [==============================] - 1s 286us/step - loss: 0.0564 - acc: 0.9944 - val_loss: 0.2144 - val_acc: 0.9464\n",
      "Epoch 131/400\n",
      "1960/1960 [==============================] - 1s 379us/step - loss: 0.0627 - acc: 0.9913 - val_loss: 0.2138 - val_acc: 0.9464\n",
      "Epoch 132/400\n",
      "1960/1960 [==============================] - 1s 271us/step - loss: 0.0587 - acc: 0.9934 - val_loss: 0.2132 - val_acc: 0.9464\n",
      "Epoch 133/400\n",
      "1960/1960 [==============================] - 1s 333us/step - loss: 0.0613 - acc: 0.9923 - val_loss: 0.2126 - val_acc: 0.9464\n",
      "Epoch 134/400\n",
      "1960/1960 [==============================] - 0s 248us/step - loss: 0.0652 - acc: 0.9929 - val_loss: 0.2121 - val_acc: 0.9464\n",
      "Epoch 135/400\n",
      "1960/1960 [==============================] - 1s 329us/step - loss: 0.0591 - acc: 0.9939 - val_loss: 0.2116 - val_acc: 0.9464\n",
      "Epoch 136/400\n",
      "1960/1960 [==============================] - 1s 283us/step - loss: 0.0582 - acc: 0.9908 - val_loss: 0.2112 - val_acc: 0.9464\n",
      "Epoch 137/400\n",
      "1960/1960 [==============================] - 1s 341us/step - loss: 0.0537 - acc: 0.9939 - val_loss: 0.2108 - val_acc: 0.9464\n",
      "Epoch 138/400\n",
      "1960/1960 [==============================] - 1s 270us/step - loss: 0.0565 - acc: 0.9944 - val_loss: 0.2105 - val_acc: 0.9464\n",
      "Epoch 139/400\n",
      "1960/1960 [==============================] - 1s 311us/step - loss: 0.0605 - acc: 0.9908 - val_loss: 0.2101 - val_acc: 0.9464\n",
      "Epoch 140/400\n",
      "1960/1960 [==============================] - 1s 264us/step - loss: 0.0546 - acc: 0.9954 - val_loss: 0.2097 - val_acc: 0.9476\n",
      "Epoch 141/400\n",
      "1960/1960 [==============================] - 1s 308us/step - loss: 0.0544 - acc: 0.9944 - val_loss: 0.2093 - val_acc: 0.9476\n",
      "Epoch 142/400\n",
      "1960/1960 [==============================] - 0s 234us/step - loss: 0.0568 - acc: 0.9918 - val_loss: 0.2090 - val_acc: 0.9476\n",
      "Epoch 143/400\n",
      "1960/1960 [==============================] - 1s 301us/step - loss: 0.0575 - acc: 0.9939 - val_loss: 0.2087 - val_acc: 0.9476\n",
      "Epoch 144/400\n",
      "1960/1960 [==============================] - 1s 296us/step - loss: 0.0593 - acc: 0.9934 - val_loss: 0.2084 - val_acc: 0.9476\n",
      "Epoch 145/400\n",
      "1960/1960 [==============================] - 1s 407us/step - loss: 0.0575 - acc: 0.9939 - val_loss: 0.2082 - val_acc: 0.9476\n",
      "Epoch 146/400\n",
      "1960/1960 [==============================] - 0s 239us/step - loss: 0.0574 - acc: 0.9949 - val_loss: 0.2080 - val_acc: 0.9464\n",
      "Epoch 147/400\n",
      "1960/1960 [==============================] - 1s 302us/step - loss: 0.0600 - acc: 0.9929 - val_loss: 0.2078 - val_acc: 0.9464\n",
      "Epoch 148/400\n",
      "1960/1960 [==============================] - 1s 262us/step - loss: 0.0605 - acc: 0.9929 - val_loss: 0.2075 - val_acc: 0.9464\n",
      "Epoch 149/400\n",
      "1960/1960 [==============================] - 1s 309us/step - loss: 0.0563 - acc: 0.9939 - val_loss: 0.2074 - val_acc: 0.9464\n",
      "Epoch 150/400\n",
      "1960/1960 [==============================] - 0s 255us/step - loss: 0.0563 - acc: 0.9934 - val_loss: 0.2072 - val_acc: 0.9464\n",
      "Epoch 151/400\n",
      "1960/1960 [==============================] - 1s 307us/step - loss: 0.0603 - acc: 0.9923 - val_loss: 0.2070 - val_acc: 0.9464\n",
      "Epoch 152/400\n",
      "1960/1960 [==============================] - 1s 295us/step - loss: 0.0559 - acc: 0.9954 - val_loss: 0.2068 - val_acc: 0.9464\n",
      "Epoch 153/400\n",
      "1960/1960 [==============================] - 1s 327us/step - loss: 0.0553 - acc: 0.9974 - val_loss: 0.2066 - val_acc: 0.9464\n",
      "Epoch 154/400\n",
      "1960/1960 [==============================] - 0s 246us/step - loss: 0.0574 - acc: 0.9913 - val_loss: 0.2064 - val_acc: 0.9464\n",
      "Epoch 155/400\n",
      "1960/1960 [==============================] - 1s 337us/step - loss: 0.0542 - acc: 0.9954 - val_loss: 0.2062 - val_acc: 0.9476\n",
      "Epoch 156/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 0.0563 - acc: 0.9939 - val_loss: 0.2060 - val_acc: 0.9476\n",
      "Epoch 157/400\n",
      "1960/1960 [==============================] - 1s 319us/step - loss: 0.0548 - acc: 0.9918 - val_loss: 0.2058 - val_acc: 0.9476\n",
      "Epoch 158/400\n",
      "1960/1960 [==============================] - 0s 250us/step - loss: 0.0588 - acc: 0.9908 - val_loss: 0.2056 - val_acc: 0.9476\n",
      "Epoch 159/400\n",
      "1960/1960 [==============================] - 1s 307us/step - loss: 0.0579 - acc: 0.9944 - val_loss: 0.2054 - val_acc: 0.9476\n",
      "Epoch 160/400\n",
      "1960/1960 [==============================] - 1s 267us/step - loss: 0.0566 - acc: 0.9969 - val_loss: 0.2052 - val_acc: 0.9476\n",
      "Epoch 161/400\n",
      "1960/1960 [==============================] - 1s 324us/step - loss: 0.0529 - acc: 0.9954 - val_loss: 0.2050 - val_acc: 0.9476\n",
      "Epoch 162/400\n",
      "1960/1960 [==============================] - 1s 299us/step - loss: 0.0541 - acc: 0.9959 - val_loss: 0.2048 - val_acc: 0.9476\n",
      "Epoch 163/400\n",
      "1960/1960 [==============================] - 1s 487us/step - loss: 0.0570 - acc: 0.9939 - val_loss: 0.2047 - val_acc: 0.9476\n",
      "Epoch 164/400\n",
      "1960/1960 [==============================] - 1s 315us/step - loss: 0.0550 - acc: 0.9934 - val_loss: 0.2045 - val_acc: 0.9476\n",
      "Epoch 165/400\n",
      "1960/1960 [==============================] - 1s 503us/step - loss: 0.0531 - acc: 0.9959 - val_loss: 0.2043 - val_acc: 0.9476\n",
      "Epoch 166/400\n",
      "1960/1960 [==============================] - 1s 387us/step - loss: 0.0570 - acc: 0.9964 - val_loss: 0.2041 - val_acc: 0.9476\n",
      "Epoch 167/400\n",
      "1960/1960 [==============================] - 1s 501us/step - loss: 0.0535 - acc: 0.9944 - val_loss: 0.2040 - val_acc: 0.9476\n",
      "Epoch 168/400\n",
      "1960/1960 [==============================] - 1s 425us/step - loss: 0.0521 - acc: 0.9959 - val_loss: 0.2038 - val_acc: 0.9476\n",
      "Epoch 169/400\n",
      "1960/1960 [==============================] - 1s 519us/step - loss: 0.0515 - acc: 0.9939 - val_loss: 0.2036 - val_acc: 0.9476\n",
      "Epoch 170/400\n",
      "1960/1960 [==============================] - 1s 362us/step - loss: 0.0542 - acc: 0.9944 - val_loss: 0.2035 - val_acc: 0.9476\n",
      "Epoch 171/400\n",
      "1960/1960 [==============================] - 1s 463us/step - loss: 0.0558 - acc: 0.9959 - val_loss: 0.2033 - val_acc: 0.9464\n",
      "Epoch 172/400\n",
      "1960/1960 [==============================] - 1s 431us/step - loss: 0.0508 - acc: 0.9929 - val_loss: 0.2031 - val_acc: 0.9464\n",
      "Epoch 173/400\n",
      "1960/1960 [==============================] - 1s 553us/step - loss: 0.0593 - acc: 0.9934 - val_loss: 0.2029 - val_acc: 0.9464\n",
      "Epoch 174/400\n",
      "1960/1960 [==============================] - 1s 447us/step - loss: 0.0536 - acc: 0.9954 - val_loss: 0.2028 - val_acc: 0.9464\n",
      "Epoch 175/400\n",
      "1960/1960 [==============================] - 1s 555us/step - loss: 0.0601 - acc: 0.9908 - val_loss: 0.2026 - val_acc: 0.9464\n",
      "Epoch 176/400\n",
      "1960/1960 [==============================] - 1s 306us/step - loss: 0.0534 - acc: 0.9949 - val_loss: 0.2025 - val_acc: 0.9464\n",
      "Epoch 177/400\n",
      "1960/1960 [==============================] - 1s 540us/step - loss: 0.0534 - acc: 0.9939 - val_loss: 0.2023 - val_acc: 0.9464\n",
      "Epoch 178/400\n",
      "1960/1960 [==============================] - 1s 415us/step - loss: 0.0523 - acc: 0.9969 - val_loss: 0.2022 - val_acc: 0.9464\n",
      "Epoch 179/400\n",
      "1960/1960 [==============================] - 1s 469us/step - loss: 0.0545 - acc: 0.9954 - val_loss: 0.2021 - val_acc: 0.9464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/400\n",
      "1960/1960 [==============================] - 1s 437us/step - loss: 0.0502 - acc: 0.9964 - val_loss: 0.2019 - val_acc: 0.9464\n",
      "Epoch 181/400\n",
      "1960/1960 [==============================] - 1s 517us/step - loss: 0.0520 - acc: 0.9954 - val_loss: 0.2018 - val_acc: 0.9464\n",
      "Epoch 182/400\n",
      "1960/1960 [==============================] - 1s 365us/step - loss: 0.0579 - acc: 0.9954 - val_loss: 0.2016 - val_acc: 0.9464\n",
      "Epoch 183/400\n",
      "1960/1960 [==============================] - 1s 534us/step - loss: 0.0552 - acc: 0.9939 - val_loss: 0.2015 - val_acc: 0.9464\n",
      "Epoch 184/400\n",
      "1960/1960 [==============================] - 1s 391us/step - loss: 0.0614 - acc: 0.9934 - val_loss: 0.2013 - val_acc: 0.9464\n",
      "Epoch 185/400\n",
      "1960/1960 [==============================] - 1s 521us/step - loss: 0.0652 - acc: 0.9918 - val_loss: 0.2012 - val_acc: 0.9464\n",
      "Epoch 186/400\n",
      "1960/1960 [==============================] - 1s 370us/step - loss: 0.0556 - acc: 0.9944 - val_loss: 0.2011 - val_acc: 0.9464\n",
      "Epoch 187/400\n",
      "1960/1960 [==============================] - 1s 510us/step - loss: 0.0539 - acc: 0.9969 - val_loss: 0.2009 - val_acc: 0.9464\n",
      "Epoch 188/400\n",
      "1960/1960 [==============================] - 1s 380us/step - loss: 0.0554 - acc: 0.9949 - val_loss: 0.2008 - val_acc: 0.9464\n",
      "Epoch 189/400\n",
      "1960/1960 [==============================] - 1s 540us/step - loss: 0.0497 - acc: 0.9954 - val_loss: 0.2007 - val_acc: 0.9464\n",
      "Epoch 190/400\n",
      "1960/1960 [==============================] - 1s 390us/step - loss: 0.0503 - acc: 0.9949 - val_loss: 0.2006 - val_acc: 0.9464\n",
      "Epoch 191/400\n",
      "1960/1960 [==============================] - 1s 505us/step - loss: 0.0564 - acc: 0.9944 - val_loss: 0.2004 - val_acc: 0.9476\n",
      "Epoch 192/400\n",
      "1960/1960 [==============================] - 1s 325us/step - loss: 0.0510 - acc: 0.9969 - val_loss: 0.2003 - val_acc: 0.9476\n",
      "Epoch 193/400\n",
      "1960/1960 [==============================] - 1s 497us/step - loss: 0.0483 - acc: 0.9964 - val_loss: 0.2002 - val_acc: 0.9476\n",
      "Epoch 194/400\n",
      "1960/1960 [==============================] - 1s 369us/step - loss: 0.0497 - acc: 0.9949 - val_loss: 0.2001 - val_acc: 0.9476\n",
      "Epoch 195/400\n",
      "1960/1960 [==============================] - 1s 475us/step - loss: 0.0622 - acc: 0.9908 - val_loss: 0.2000 - val_acc: 0.9464\n",
      "Epoch 196/400\n",
      "1960/1960 [==============================] - 1s 325us/step - loss: 0.0541 - acc: 0.9934 - val_loss: 0.1999 - val_acc: 0.9464\n",
      "Epoch 197/400\n",
      "1960/1960 [==============================] - 1s 438us/step - loss: 0.0606 - acc: 0.9898 - val_loss: 0.1998 - val_acc: 0.9464\n",
      "Epoch 198/400\n",
      "1960/1960 [==============================] - 1s 399us/step - loss: 0.0585 - acc: 0.9939 - val_loss: 0.1997 - val_acc: 0.9464\n",
      "Epoch 199/400\n",
      "1960/1960 [==============================] - 1s 534us/step - loss: 0.0480 - acc: 0.9974 - val_loss: 0.1996 - val_acc: 0.9464\n",
      "Epoch 200/400\n",
      "1960/1960 [==============================] - 1s 449us/step - loss: 0.0464 - acc: 0.9959 - val_loss: 0.1996 - val_acc: 0.9464\n",
      "Epoch 201/400\n",
      "1960/1960 [==============================] - 1s 449us/step - loss: 0.0610 - acc: 0.9939 - val_loss: 0.1995 - val_acc: 0.9464\n",
      "Epoch 202/400\n",
      "1960/1960 [==============================] - 1s 351us/step - loss: 0.0567 - acc: 0.9959 - val_loss: 0.1994 - val_acc: 0.9464\n",
      "Epoch 203/400\n",
      "1960/1960 [==============================] - 1s 480us/step - loss: 0.0606 - acc: 0.9898 - val_loss: 0.1994 - val_acc: 0.9464\n",
      "Epoch 204/400\n",
      "1960/1960 [==============================] - 1s 397us/step - loss: 0.0555 - acc: 0.9964 - val_loss: 0.1993 - val_acc: 0.9464\n",
      "Epoch 205/400\n",
      "1960/1960 [==============================] - 1s 517us/step - loss: 0.0534 - acc: 0.9944 - val_loss: 0.1992 - val_acc: 0.9464\n",
      "Epoch 206/400\n",
      "1960/1960 [==============================] - 1s 309us/step - loss: 0.0559 - acc: 0.9944 - val_loss: 0.1992 - val_acc: 0.9464\n",
      "Epoch 207/400\n",
      "1960/1960 [==============================] - 1s 491us/step - loss: 0.0492 - acc: 0.9959 - val_loss: 0.1991 - val_acc: 0.9464\n",
      "Epoch 208/400\n",
      "1960/1960 [==============================] - 1s 401us/step - loss: 0.0465 - acc: 0.9964 - val_loss: 0.1990 - val_acc: 0.9464\n",
      "Epoch 209/400\n",
      "1960/1960 [==============================] - 1s 491us/step - loss: 0.0588 - acc: 0.9923 - val_loss: 0.1990 - val_acc: 0.9464\n",
      "Epoch 210/400\n",
      "1960/1960 [==============================] - 1s 375us/step - loss: 0.0579 - acc: 0.9949 - val_loss: 0.1989 - val_acc: 0.9464\n",
      "Epoch 211/400\n",
      "1960/1960 [==============================] - 1s 502us/step - loss: 0.0548 - acc: 0.9949 - val_loss: 0.1989 - val_acc: 0.9464\n",
      "Epoch 212/400\n",
      "1960/1960 [==============================] - 1s 431us/step - loss: 0.0586 - acc: 0.9923 - val_loss: 0.1988 - val_acc: 0.9464\n",
      "Epoch 213/400\n",
      "1960/1960 [==============================] - 1s 462us/step - loss: 0.0561 - acc: 0.9949 - val_loss: 0.1988 - val_acc: 0.9464\n",
      "Epoch 214/400\n",
      "1960/1960 [==============================] - 1s 370us/step - loss: 0.0556 - acc: 0.9954 - val_loss: 0.1988 - val_acc: 0.9464\n",
      "Epoch 215/400\n",
      "1960/1960 [==============================] - 1s 505us/step - loss: 0.0543 - acc: 0.9954 - val_loss: 0.1987 - val_acc: 0.9464\n",
      "Epoch 216/400\n",
      "1960/1960 [==============================] - 1s 313us/step - loss: 0.0556 - acc: 0.9939 - val_loss: 0.1987 - val_acc: 0.9464\n",
      "Epoch 217/400\n",
      "1960/1960 [==============================] - 1s 491us/step - loss: 0.0541 - acc: 0.9949 - val_loss: 0.1986 - val_acc: 0.9464\n",
      "Epoch 218/400\n",
      "1960/1960 [==============================] - 1s 417us/step - loss: 0.0538 - acc: 0.9944 - val_loss: 0.1986 - val_acc: 0.9452\n",
      "Epoch 219/400\n",
      "1960/1960 [==============================] - 1s 411us/step - loss: 0.0568 - acc: 0.9954 - val_loss: 0.1986 - val_acc: 0.9452\n",
      "Epoch 220/400\n",
      "1960/1960 [==============================] - 1s 449us/step - loss: 0.0537 - acc: 0.9944 - val_loss: 0.1985 - val_acc: 0.9452\n",
      "Epoch 221/400\n",
      "1960/1960 [==============================] - 1s 564us/step - loss: 0.0527 - acc: 0.9974 - val_loss: 0.1985 - val_acc: 0.9464\n",
      "Epoch 222/400\n",
      "1960/1960 [==============================] - 1s 399us/step - loss: 0.0475 - acc: 0.9959 - val_loss: 0.1984 - val_acc: 0.9464\n",
      "Epoch 223/400\n",
      "1960/1960 [==============================] - 1s 455us/step - loss: 0.0582 - acc: 0.9908 - val_loss: 0.1984 - val_acc: 0.9464\n",
      "Epoch 224/400\n",
      "1960/1960 [==============================] - 1s 397us/step - loss: 0.0592 - acc: 0.9934 - val_loss: 0.1983 - val_acc: 0.9464\n",
      "Epoch 225/400\n",
      "1960/1960 [==============================] - 1s 377us/step - loss: 0.0535 - acc: 0.9954 - val_loss: 0.1983 - val_acc: 0.9464\n",
      "Epoch 226/400\n",
      "1960/1960 [==============================] - 1s 273us/step - loss: 0.0564 - acc: 0.9944 - val_loss: 0.1982 - val_acc: 0.9464\n",
      "Epoch 227/400\n",
      "1960/1960 [==============================] - 1s 521us/step - loss: 0.0513 - acc: 0.9949 - val_loss: 0.1982 - val_acc: 0.9464\n",
      "Epoch 228/400\n",
      "1960/1960 [==============================] - 1s 418us/step - loss: 0.0597 - acc: 0.9934 - val_loss: 0.1981 - val_acc: 0.9452\n",
      "Epoch 229/400\n",
      "1960/1960 [==============================] - 1s 441us/step - loss: 0.0519 - acc: 0.9923 - val_loss: 0.1980 - val_acc: 0.9452\n",
      "Epoch 230/400\n",
      "1960/1960 [==============================] - 1s 406us/step - loss: 0.0573 - acc: 0.9939 - val_loss: 0.1980 - val_acc: 0.9452\n",
      "Epoch 231/400\n",
      "1960/1960 [==============================] - 1s 524us/step - loss: 0.0522 - acc: 0.9959 - val_loss: 0.1979 - val_acc: 0.9452\n",
      "Epoch 232/400\n",
      "1960/1960 [==============================] - 1s 376us/step - loss: 0.0546 - acc: 0.9923 - val_loss: 0.1978 - val_acc: 0.9452\n",
      "Epoch 233/400\n",
      "1960/1960 [==============================] - 1s 474us/step - loss: 0.0531 - acc: 0.9949 - val_loss: 0.1978 - val_acc: 0.9452\n",
      "Epoch 234/400\n",
      "1960/1960 [==============================] - 1s 387us/step - loss: 0.0581 - acc: 0.9929 - val_loss: 0.1977 - val_acc: 0.9452\n",
      "Epoch 235/400\n",
      "1960/1960 [==============================] - 1s 509us/step - loss: 0.0576 - acc: 0.9949 - val_loss: 0.1976 - val_acc: 0.9452\n",
      "Epoch 236/400\n",
      "1960/1960 [==============================] - 1s 385us/step - loss: 0.0522 - acc: 0.9949 - val_loss: 0.1975 - val_acc: 0.9452\n",
      "Epoch 237/400\n",
      "1960/1960 [==============================] - 1s 519us/step - loss: 0.0555 - acc: 0.9949 - val_loss: 0.1975 - val_acc: 0.9452\n",
      "Epoch 238/400\n",
      "1960/1960 [==============================] - 1s 361us/step - loss: 0.0555 - acc: 0.9923 - val_loss: 0.1974 - val_acc: 0.9452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/400\n",
      "1960/1960 [==============================] - 1s 434us/step - loss: 0.0525 - acc: 0.9944 - val_loss: 0.1973 - val_acc: 0.9452\n",
      "Epoch 240/400\n",
      "1960/1960 [==============================] - 1s 366us/step - loss: 0.0545 - acc: 0.9969 - val_loss: 0.1973 - val_acc: 0.9452\n",
      "Epoch 241/400\n",
      "1960/1960 [==============================] - 1s 479us/step - loss: 0.0516 - acc: 0.9964 - val_loss: 0.1972 - val_acc: 0.9452\n",
      "Epoch 242/400\n",
      "1960/1960 [==============================] - 1s 408us/step - loss: 0.0584 - acc: 0.9934 - val_loss: 0.1971 - val_acc: 0.9452\n",
      "Epoch 243/400\n",
      "1960/1960 [==============================] - 1s 514us/step - loss: 0.0609 - acc: 0.9939 - val_loss: 0.1970 - val_acc: 0.9452\n",
      "Epoch 244/400\n",
      "1960/1960 [==============================] - 1s 470us/step - loss: 0.0556 - acc: 0.9934 - val_loss: 0.1969 - val_acc: 0.9452\n",
      "Epoch 245/400\n",
      "1960/1960 [==============================] - 1s 611us/step - loss: 0.0591 - acc: 0.9939 - val_loss: 0.1968 - val_acc: 0.9452\n",
      "Epoch 246/400\n",
      "1960/1960 [==============================] - 1s 426us/step - loss: 0.0590 - acc: 0.9939 - val_loss: 0.1967 - val_acc: 0.9464\n",
      "Epoch 247/400\n",
      "1960/1960 [==============================] - 1s 530us/step - loss: 0.0612 - acc: 0.9929 - val_loss: 0.1967 - val_acc: 0.9464\n",
      "Epoch 248/400\n",
      "1960/1960 [==============================] - 1s 421us/step - loss: 0.0560 - acc: 0.9949 - val_loss: 0.1966 - val_acc: 0.9464\n",
      "Epoch 249/400\n",
      "1960/1960 [==============================] - 1s 525us/step - loss: 0.0536 - acc: 0.9939 - val_loss: 0.1965 - val_acc: 0.9464\n",
      "Epoch 250/400\n",
      "1960/1960 [==============================] - 1s 358us/step - loss: 0.0500 - acc: 0.9944 - val_loss: 0.1964 - val_acc: 0.9464\n",
      "Epoch 251/400\n",
      "1960/1960 [==============================] - 1s 375us/step - loss: 0.0500 - acc: 0.9949 - val_loss: 0.1963 - val_acc: 0.9464\n",
      "Epoch 252/400\n",
      "1960/1960 [==============================] - 1s 382us/step - loss: 0.0560 - acc: 0.9939 - val_loss: 0.1962 - val_acc: 0.9464\n",
      "Epoch 253/400\n",
      "1960/1960 [==============================] - 1s 481us/step - loss: 0.0514 - acc: 0.9954 - val_loss: 0.1961 - val_acc: 0.9464\n",
      "Epoch 254/400\n",
      "1960/1960 [==============================] - 1s 361us/step - loss: 0.0539 - acc: 0.9939 - val_loss: 0.1960 - val_acc: 0.9464\n",
      "Epoch 255/400\n",
      "1960/1960 [==============================] - 1s 460us/step - loss: 0.0558 - acc: 0.9918 - val_loss: 0.1960 - val_acc: 0.9464\n",
      "Epoch 256/400\n",
      "1960/1960 [==============================] - 1s 316us/step - loss: 0.0571 - acc: 0.9923 - val_loss: 0.1959 - val_acc: 0.9464\n",
      "Epoch 257/400\n",
      "1960/1960 [==============================] - 1s 571us/step - loss: 0.0539 - acc: 0.9944 - val_loss: 0.1958 - val_acc: 0.9464\n",
      "Epoch 258/400\n",
      "1960/1960 [==============================] - 1s 372us/step - loss: 0.0495 - acc: 0.9954 - val_loss: 0.1957 - val_acc: 0.9464\n",
      "Epoch 259/400\n",
      "1960/1960 [==============================] - 1s 529us/step - loss: 0.0560 - acc: 0.9949 - val_loss: 0.1957 - val_acc: 0.9464\n",
      "Epoch 260/400\n",
      "1960/1960 [==============================] - 1s 418us/step - loss: 0.0516 - acc: 0.9929 - val_loss: 0.1956 - val_acc: 0.9464\n",
      "Epoch 261/400\n",
      "1960/1960 [==============================] - 1s 547us/step - loss: 0.0538 - acc: 0.9913 - val_loss: 0.1956 - val_acc: 0.9464\n",
      "Epoch 262/400\n",
      "1960/1960 [==============================] - 1s 415us/step - loss: 0.0564 - acc: 0.9934 - val_loss: 0.1955 - val_acc: 0.9464\n",
      "Epoch 263/400\n",
      "1960/1960 [==============================] - 1s 521us/step - loss: 0.0566 - acc: 0.9923 - val_loss: 0.1954 - val_acc: 0.9464\n",
      "Epoch 264/400\n",
      "1960/1960 [==============================] - 1s 433us/step - loss: 0.0554 - acc: 0.9949 - val_loss: 0.1954 - val_acc: 0.9464\n",
      "Epoch 265/400\n",
      "1960/1960 [==============================] - 1s 527us/step - loss: 0.0539 - acc: 0.9954 - val_loss: 0.1953 - val_acc: 0.9464\n",
      "Epoch 266/400\n",
      "1960/1960 [==============================] - 1s 359us/step - loss: 0.0555 - acc: 0.9959 - val_loss: 0.1953 - val_acc: 0.9464\n",
      "Epoch 267/400\n",
      "1960/1960 [==============================] - 1s 497us/step - loss: 0.0535 - acc: 0.9959 - val_loss: 0.1952 - val_acc: 0.9464\n",
      "Epoch 268/400\n",
      "1960/1960 [==============================] - 1s 394us/step - loss: 0.0569 - acc: 0.9929 - val_loss: 0.1952 - val_acc: 0.9464\n",
      "Epoch 269/400\n",
      "1960/1960 [==============================] - 1s 508us/step - loss: 0.0565 - acc: 0.9934 - val_loss: 0.1951 - val_acc: 0.9464\n",
      "Epoch 270/400\n",
      "1960/1960 [==============================] - 1s 382us/step - loss: 0.0516 - acc: 0.9959 - val_loss: 0.1950 - val_acc: 0.9464\n",
      "Epoch 271/400\n",
      "1960/1960 [==============================] - 1s 417us/step - loss: 0.0516 - acc: 0.9964 - val_loss: 0.1950 - val_acc: 0.9464\n",
      "Epoch 272/400\n",
      "1960/1960 [==============================] - 1s 389us/step - loss: 0.0594 - acc: 0.9918 - val_loss: 0.1949 - val_acc: 0.9464\n",
      "Epoch 273/400\n",
      "1960/1960 [==============================] - 1s 501us/step - loss: 0.0565 - acc: 0.9954 - val_loss: 0.1949 - val_acc: 0.9464\n",
      "Epoch 274/400\n",
      "1960/1960 [==============================] - 1s 426us/step - loss: 0.0589 - acc: 0.9939 - val_loss: 0.1948 - val_acc: 0.9464\n",
      "Epoch 275/400\n",
      "1960/1960 [==============================] - 1s 446us/step - loss: 0.0507 - acc: 0.9954 - val_loss: 0.1948 - val_acc: 0.9464\n",
      "Epoch 276/400\n",
      "1960/1960 [==============================] - 1s 406us/step - loss: 0.0570 - acc: 0.9949 - val_loss: 0.1948 - val_acc: 0.9464\n",
      "Epoch 277/400\n",
      "1960/1960 [==============================] - 1s 549us/step - loss: 0.0509 - acc: 0.9954 - val_loss: 0.1948 - val_acc: 0.9464\n",
      "Epoch 278/400\n",
      "1960/1960 [==============================] - 1s 359us/step - loss: 0.0543 - acc: 0.9949 - val_loss: 0.1947 - val_acc: 0.9476\n",
      "Epoch 279/400\n",
      "1960/1960 [==============================] - 1s 427us/step - loss: 0.0541 - acc: 0.9929 - val_loss: 0.1947 - val_acc: 0.9476\n",
      "Epoch 280/400\n",
      "1960/1960 [==============================] - 1s 403us/step - loss: 0.0582 - acc: 0.9939 - val_loss: 0.1947 - val_acc: 0.9476\n",
      "Epoch 281/400\n",
      "1960/1960 [==============================] - 1s 411us/step - loss: 0.0543 - acc: 0.9949 - val_loss: 0.1946 - val_acc: 0.9476\n",
      "Epoch 282/400\n",
      "1960/1960 [==============================] - 1s 356us/step - loss: 0.0478 - acc: 0.9980 - val_loss: 0.1946 - val_acc: 0.9476\n",
      "Epoch 283/400\n",
      "1960/1960 [==============================] - 1s 497us/step - loss: 0.0555 - acc: 0.9954 - val_loss: 0.1946 - val_acc: 0.9476\n",
      "Epoch 284/400\n",
      "1960/1960 [==============================] - 1s 402us/step - loss: 0.0585 - acc: 0.9918 - val_loss: 0.1945 - val_acc: 0.9476\n",
      "Epoch 285/400\n",
      "1960/1960 [==============================] - 1s 484us/step - loss: 0.0559 - acc: 0.9949 - val_loss: 0.1945 - val_acc: 0.9476\n",
      "Epoch 286/400\n",
      "1960/1960 [==============================] - 1s 406us/step - loss: 0.0523 - acc: 0.9944 - val_loss: 0.1945 - val_acc: 0.9476\n",
      "Epoch 287/400\n",
      "1960/1960 [==============================] - 1s 482us/step - loss: 0.0558 - acc: 0.9929 - val_loss: 0.1945 - val_acc: 0.9476\n",
      "Epoch 288/400\n",
      "1960/1960 [==============================] - 1s 438us/step - loss: 0.0556 - acc: 0.9949 - val_loss: 0.1944 - val_acc: 0.9476\n",
      "Epoch 289/400\n",
      "1960/1960 [==============================] - 1s 560us/step - loss: 0.0573 - acc: 0.9908 - val_loss: 0.1944 - val_acc: 0.9476\n",
      "Epoch 290/400\n",
      "1960/1960 [==============================] - 1s 411us/step - loss: 0.0545 - acc: 0.9959 - val_loss: 0.1943 - val_acc: 0.9476\n",
      "Epoch 291/400\n",
      "1960/1960 [==============================] - 1s 444us/step - loss: 0.0587 - acc: 0.9939 - val_loss: 0.1943 - val_acc: 0.9476\n",
      "Epoch 292/400\n",
      "1960/1960 [==============================] - 1s 403us/step - loss: 0.0568 - acc: 0.9939 - val_loss: 0.1943 - val_acc: 0.9476\n",
      "Epoch 293/400\n",
      "1960/1960 [==============================] - 1s 552us/step - loss: 0.0555 - acc: 0.9934 - val_loss: 0.1943 - val_acc: 0.9476\n",
      "Epoch 294/400\n",
      "1960/1960 [==============================] - 1s 389us/step - loss: 0.0538 - acc: 0.9929 - val_loss: 0.1943 - val_acc: 0.9476\n",
      "Epoch 295/400\n",
      "1960/1960 [==============================] - 1s 560us/step - loss: 0.0517 - acc: 0.9944 - val_loss: 0.1943 - val_acc: 0.9476\n",
      "Epoch 296/400\n",
      "1960/1960 [==============================] - 1s 383us/step - loss: 0.0548 - acc: 0.9959 - val_loss: 0.1943 - val_acc: 0.9476\n",
      "Epoch 297/400\n",
      "1960/1960 [==============================] - 1s 465us/step - loss: 0.0554 - acc: 0.9934 - val_loss: 0.1942 - val_acc: 0.9476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298/400\n",
      "1960/1960 [==============================] - 1s 399us/step - loss: 0.0546 - acc: 0.9959 - val_loss: 0.1942 - val_acc: 0.9476\n",
      "Epoch 299/400\n",
      "1960/1960 [==============================] - 1s 499us/step - loss: 0.0569 - acc: 0.9939 - val_loss: 0.1942 - val_acc: 0.9476\n",
      "Epoch 300/400\n",
      "1960/1960 [==============================] - 1s 480us/step - loss: 0.0588 - acc: 0.9918 - val_loss: 0.1942 - val_acc: 0.9476\n",
      "Epoch 301/400\n",
      "1960/1960 [==============================] - 1s 594us/step - loss: 0.0526 - acc: 0.9954 - val_loss: 0.1942 - val_acc: 0.9476\n",
      "Epoch 302/400\n",
      "1960/1960 [==============================] - 1s 376us/step - loss: 0.0518 - acc: 0.9949 - val_loss: 0.1941 - val_acc: 0.9476\n",
      "Epoch 303/400\n",
      "1960/1960 [==============================] - 1s 474us/step - loss: 0.0593 - acc: 0.9939 - val_loss: 0.1941 - val_acc: 0.9476\n",
      "Epoch 304/400\n",
      "1960/1960 [==============================] - 1s 405us/step - loss: 0.0576 - acc: 0.9939 - val_loss: 0.1941 - val_acc: 0.9476\n",
      "Epoch 305/400\n",
      "1960/1960 [==============================] - 1s 489us/step - loss: 0.0554 - acc: 0.9944 - val_loss: 0.1940 - val_acc: 0.9476\n",
      "Epoch 306/400\n",
      "1960/1960 [==============================] - 1s 398us/step - loss: 0.0576 - acc: 0.9939 - val_loss: 0.1940 - val_acc: 0.9476\n",
      "Epoch 307/400\n",
      "1960/1960 [==============================] - 1s 493us/step - loss: 0.0599 - acc: 0.9918 - val_loss: 0.1940 - val_acc: 0.9476\n",
      "Epoch 308/400\n",
      "1960/1960 [==============================] - 1s 413us/step - loss: 0.0510 - acc: 0.9954 - val_loss: 0.1940 - val_acc: 0.9476\n",
      "Epoch 309/400\n",
      "1960/1960 [==============================] - 1s 521us/step - loss: 0.0586 - acc: 0.9959 - val_loss: 0.1939 - val_acc: 0.9476\n",
      "Epoch 310/400\n",
      "1960/1960 [==============================] - 1s 420us/step - loss: 0.0519 - acc: 0.9939 - val_loss: 0.1939 - val_acc: 0.9476\n",
      "Epoch 311/400\n",
      "1960/1960 [==============================] - 1s 525us/step - loss: 0.0479 - acc: 0.9959 - val_loss: 0.1939 - val_acc: 0.9476\n",
      "Epoch 312/400\n",
      "1960/1960 [==============================] - 1s 323us/step - loss: 0.0570 - acc: 0.9939 - val_loss: 0.1939 - val_acc: 0.9476\n",
      "Epoch 313/400\n",
      "1960/1960 [==============================] - 1s 268us/step - loss: 0.0472 - acc: 0.9969 - val_loss: 0.1939 - val_acc: 0.9476\n",
      "Epoch 314/400\n",
      "1960/1960 [==============================] - 0s 239us/step - loss: 0.0587 - acc: 0.9939 - val_loss: 0.1938 - val_acc: 0.9476\n",
      "Epoch 315/400\n",
      "1960/1960 [==============================] - 0s 233us/step - loss: 0.0587 - acc: 0.9929 - val_loss: 0.1938 - val_acc: 0.9476\n",
      "Epoch 316/400\n",
      "1960/1960 [==============================] - 1s 316us/step - loss: 0.0605 - acc: 0.9939 - val_loss: 0.1938 - val_acc: 0.9476\n",
      "Epoch 317/400\n",
      "1960/1960 [==============================] - 1s 257us/step - loss: 0.0572 - acc: 0.9934 - val_loss: 0.1938 - val_acc: 0.9476\n",
      "Epoch 318/400\n",
      "1960/1960 [==============================] - 0s 207us/step - loss: 0.0485 - acc: 0.9954 - val_loss: 0.1938 - val_acc: 0.9476\n",
      "Epoch 319/400\n",
      "1960/1960 [==============================] - 0s 229us/step - loss: 0.0560 - acc: 0.9929 - val_loss: 0.1937 - val_acc: 0.9476\n",
      "Epoch 320/400\n",
      "1960/1960 [==============================] - 0s 238us/step - loss: 0.0544 - acc: 0.9944 - val_loss: 0.1937 - val_acc: 0.9476\n",
      "Epoch 321/400\n",
      "1960/1960 [==============================] - 0s 247us/step - loss: 0.0572 - acc: 0.9939 - val_loss: 0.1937 - val_acc: 0.9476\n",
      "Epoch 322/400\n",
      "1960/1960 [==============================] - 0s 214us/step - loss: 0.0524 - acc: 0.9954 - val_loss: 0.1937 - val_acc: 0.9476\n",
      "Epoch 323/400\n",
      "1960/1960 [==============================] - 0s 209us/step - loss: 0.0571 - acc: 0.9949 - val_loss: 0.1936 - val_acc: 0.9476\n",
      "Epoch 324/400\n",
      "1960/1960 [==============================] - 0s 223us/step - loss: 0.0451 - acc: 0.9969 - val_loss: 0.1936 - val_acc: 0.9476\n",
      "Epoch 325/400\n",
      "1960/1960 [==============================] - 0s 233us/step - loss: 0.0565 - acc: 0.9959 - val_loss: 0.1936 - val_acc: 0.9476\n",
      "Epoch 326/400\n",
      "1960/1960 [==============================] - 0s 232us/step - loss: 0.0544 - acc: 0.9929 - val_loss: 0.1935 - val_acc: 0.9476\n",
      "Epoch 327/400\n",
      "1960/1960 [==============================] - 0s 236us/step - loss: 0.0534 - acc: 0.9959 - val_loss: 0.1935 - val_acc: 0.9476\n",
      "Epoch 328/400\n",
      "1960/1960 [==============================] - 0s 230us/step - loss: 0.0558 - acc: 0.9954 - val_loss: 0.1934 - val_acc: 0.9476\n",
      "Epoch 329/400\n",
      "1960/1960 [==============================] - 1s 260us/step - loss: 0.0536 - acc: 0.9959 - val_loss: 0.1934 - val_acc: 0.9476\n",
      "Epoch 330/400\n",
      "1960/1960 [==============================] - 1s 258us/step - loss: 0.0551 - acc: 0.9954 - val_loss: 0.1934 - val_acc: 0.9476\n",
      "Epoch 331/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 0.0537 - acc: 0.9944 - val_loss: 0.1933 - val_acc: 0.9476\n",
      "Epoch 332/400\n",
      "1960/1960 [==============================] - 1s 269us/step - loss: 0.0574 - acc: 0.9934 - val_loss: 0.1933 - val_acc: 0.9476\n",
      "Epoch 333/400\n",
      "1960/1960 [==============================] - 1s 291us/step - loss: 0.0506 - acc: 0.9959 - val_loss: 0.1933 - val_acc: 0.9476\n",
      "Epoch 334/400\n",
      "1960/1960 [==============================] - 0s 224us/step - loss: 0.0556 - acc: 0.9929 - val_loss: 0.1933 - val_acc: 0.9476\n",
      "Epoch 335/400\n",
      "1960/1960 [==============================] - 1s 268us/step - loss: 0.0492 - acc: 0.9969 - val_loss: 0.1933 - val_acc: 0.9476\n",
      "Epoch 336/400\n",
      "1960/1960 [==============================] - 0s 253us/step - loss: 0.0540 - acc: 0.9954 - val_loss: 0.1932 - val_acc: 0.9476\n",
      "Epoch 337/400\n",
      "1960/1960 [==============================] - 1s 276us/step - loss: 0.0649 - acc: 0.9898 - val_loss: 0.1932 - val_acc: 0.9476\n",
      "Epoch 338/400\n",
      "1960/1960 [==============================] - 0s 232us/step - loss: 0.0527 - acc: 0.9929 - val_loss: 0.1932 - val_acc: 0.9476\n",
      "Epoch 339/400\n",
      "1960/1960 [==============================] - 1s 278us/step - loss: 0.0497 - acc: 0.9964 - val_loss: 0.1932 - val_acc: 0.9476\n",
      "Epoch 340/400\n",
      "1960/1960 [==============================] - 0s 218us/step - loss: 0.0512 - acc: 0.9949 - val_loss: 0.1933 - val_acc: 0.9464\n",
      "Epoch 341/400\n",
      "1960/1960 [==============================] - 1s 288us/step - loss: 0.0580 - acc: 0.9939 - val_loss: 0.1933 - val_acc: 0.9464\n",
      "Epoch 342/400\n",
      "1960/1960 [==============================] - 0s 215us/step - loss: 0.0517 - acc: 0.9959 - val_loss: 0.1933 - val_acc: 0.9464\n",
      "Epoch 343/400\n",
      "1960/1960 [==============================] - 1s 285us/step - loss: 0.0495 - acc: 0.9929 - val_loss: 0.1933 - val_acc: 0.9464\n",
      "Epoch 344/400\n",
      "1960/1960 [==============================] - 0s 225us/step - loss: 0.0530 - acc: 0.9944 - val_loss: 0.1933 - val_acc: 0.9464\n",
      "Epoch 345/400\n",
      "1960/1960 [==============================] - 1s 263us/step - loss: 0.0552 - acc: 0.9929 - val_loss: 0.1933 - val_acc: 0.9464\n",
      "Epoch 346/400\n",
      "1960/1960 [==============================] - 1s 292us/step - loss: 0.0501 - acc: 0.9969 - val_loss: 0.1933 - val_acc: 0.9464\n",
      "Epoch 347/400\n",
      "1960/1960 [==============================] - 1s 270us/step - loss: 0.0602 - acc: 0.9913 - val_loss: 0.1932 - val_acc: 0.9464\n",
      "Epoch 348/400\n",
      "1960/1960 [==============================] - 0s 254us/step - loss: 0.0523 - acc: 0.9949 - val_loss: 0.1932 - val_acc: 0.9464\n",
      "Epoch 349/400\n",
      "1960/1960 [==============================] - 1s 291us/step - loss: 0.0569 - acc: 0.9918 - val_loss: 0.1932 - val_acc: 0.9464\n",
      "Epoch 350/400\n",
      "1960/1960 [==============================] - 0s 241us/step - loss: 0.0555 - acc: 0.9964 - val_loss: 0.1931 - val_acc: 0.9464\n",
      "Epoch 351/400\n",
      "1960/1960 [==============================] - 0s 240us/step - loss: 0.0542 - acc: 0.9949 - val_loss: 0.1931 - val_acc: 0.9464\n",
      "Epoch 352/400\n",
      "1960/1960 [==============================] - 0s 244us/step - loss: 0.0519 - acc: 0.9944 - val_loss: 0.1931 - val_acc: 0.9464\n",
      "Epoch 353/400\n",
      "1960/1960 [==============================] - 1s 289us/step - loss: 0.0538 - acc: 0.9964 - val_loss: 0.1931 - val_acc: 0.9464\n",
      "Epoch 354/400\n",
      "1960/1960 [==============================] - 0s 227us/step - loss: 0.0537 - acc: 0.9934 - val_loss: 0.1930 - val_acc: 0.9464\n",
      "Epoch 355/400\n",
      "1960/1960 [==============================] - 1s 279us/step - loss: 0.0546 - acc: 0.9944 - val_loss: 0.1930 - val_acc: 0.9464\n",
      "Epoch 356/400\n",
      "1960/1960 [==============================] - 0s 253us/step - loss: 0.0567 - acc: 0.9929 - val_loss: 0.1930 - val_acc: 0.9464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 357/400\n",
      "1960/1960 [==============================] - 1s 305us/step - loss: 0.0600 - acc: 0.9934 - val_loss: 0.1930 - val_acc: 0.9464\n",
      "Epoch 358/400\n",
      "1960/1960 [==============================] - 0s 251us/step - loss: 0.0563 - acc: 0.9954 - val_loss: 0.1929 - val_acc: 0.9464\n",
      "Epoch 359/400\n",
      "1960/1960 [==============================] - 0s 241us/step - loss: 0.0610 - acc: 0.9913 - val_loss: 0.1929 - val_acc: 0.9476\n",
      "Epoch 360/400\n",
      "1960/1960 [==============================] - 0s 255us/step - loss: 0.0492 - acc: 0.9949 - val_loss: 0.1929 - val_acc: 0.9476\n",
      "Epoch 361/400\n",
      "1960/1960 [==============================] - 1s 275us/step - loss: 0.0527 - acc: 0.9939 - val_loss: 0.1929 - val_acc: 0.9476\n",
      "Epoch 362/400\n",
      "1960/1960 [==============================] - 0s 224us/step - loss: 0.0571 - acc: 0.9929 - val_loss: 0.1928 - val_acc: 0.9476\n",
      "Epoch 363/400\n",
      "1960/1960 [==============================] - 1s 259us/step - loss: 0.0517 - acc: 0.9959 - val_loss: 0.1928 - val_acc: 0.9476\n",
      "Epoch 364/400\n",
      "1960/1960 [==============================] - 1s 305us/step - loss: 0.0589 - acc: 0.9903 - val_loss: 0.1927 - val_acc: 0.9476\n",
      "Epoch 365/400\n",
      "1960/1960 [==============================] - 1s 269us/step - loss: 0.0559 - acc: 0.9944 - val_loss: 0.1927 - val_acc: 0.9476\n",
      "Epoch 366/400\n",
      "1960/1960 [==============================] - 0s 237us/step - loss: 0.0561 - acc: 0.9934 - val_loss: 0.1927 - val_acc: 0.9476\n",
      "Epoch 367/400\n",
      "1960/1960 [==============================] - 1s 277us/step - loss: 0.0490 - acc: 0.9969 - val_loss: 0.1927 - val_acc: 0.9476\n",
      "Epoch 368/400\n",
      "1960/1960 [==============================] - 0s 233us/step - loss: 0.0469 - acc: 0.9954 - val_loss: 0.1926 - val_acc: 0.9476\n",
      "Epoch 369/400\n",
      "1960/1960 [==============================] - 1s 282us/step - loss: 0.0519 - acc: 0.9964 - val_loss: 0.1926 - val_acc: 0.9476\n",
      "Epoch 370/400\n",
      "1960/1960 [==============================] - 0s 234us/step - loss: 0.0527 - acc: 0.9944 - val_loss: 0.1926 - val_acc: 0.9476\n",
      "Epoch 371/400\n",
      "1960/1960 [==============================] - 1s 273us/step - loss: 0.0536 - acc: 0.9939 - val_loss: 0.1926 - val_acc: 0.9476\n",
      "Epoch 372/400\n",
      "1960/1960 [==============================] - 0s 230us/step - loss: 0.0485 - acc: 0.9949 - val_loss: 0.1926 - val_acc: 0.9476\n",
      "Epoch 373/400\n",
      "1960/1960 [==============================] - 1s 262us/step - loss: 0.0542 - acc: 0.9964 - val_loss: 0.1926 - val_acc: 0.9476\n",
      "Epoch 374/400\n",
      "1960/1960 [==============================] - 0s 227us/step - loss: 0.0534 - acc: 0.9944 - val_loss: 0.1925 - val_acc: 0.9476\n",
      "Epoch 375/400\n",
      "1960/1960 [==============================] - 1s 273us/step - loss: 0.0555 - acc: 0.9934 - val_loss: 0.1925 - val_acc: 0.9476\n",
      "Epoch 376/400\n",
      "1960/1960 [==============================] - 0s 231us/step - loss: 0.0548 - acc: 0.9939 - val_loss: 0.1925 - val_acc: 0.9476\n",
      "Epoch 377/400\n",
      "1960/1960 [==============================] - 1s 267us/step - loss: 0.0556 - acc: 0.9939 - val_loss: 0.1925 - val_acc: 0.9476\n",
      "Epoch 378/400\n",
      "1960/1960 [==============================] - 0s 239us/step - loss: 0.0545 - acc: 0.9923 - val_loss: 0.1925 - val_acc: 0.9476\n",
      "Epoch 379/400\n",
      "1960/1960 [==============================] - 1s 265us/step - loss: 0.0599 - acc: 0.9949 - val_loss: 0.1925 - val_acc: 0.9476\n",
      "Epoch 380/400\n",
      "1960/1960 [==============================] - 0s 246us/step - loss: 0.0539 - acc: 0.9944 - val_loss: 0.1924 - val_acc: 0.9476\n",
      "Epoch 381/400\n",
      "1960/1960 [==============================] - 1s 266us/step - loss: 0.0472 - acc: 0.9969 - val_loss: 0.1924 - val_acc: 0.9476\n",
      "Epoch 382/400\n",
      "1960/1960 [==============================] - 0s 204us/step - loss: 0.0536 - acc: 0.9954 - val_loss: 0.1924 - val_acc: 0.9476\n",
      "Epoch 383/400\n",
      "1960/1960 [==============================] - 1s 264us/step - loss: 0.0558 - acc: 0.9934 - val_loss: 0.1924 - val_acc: 0.9476\n",
      "Epoch 384/400\n",
      "1960/1960 [==============================] - 0s 234us/step - loss: 0.0527 - acc: 0.9959 - val_loss: 0.1924 - val_acc: 0.9476\n",
      "Epoch 385/400\n",
      "1960/1960 [==============================] - 1s 270us/step - loss: 0.0526 - acc: 0.9959 - val_loss: 0.1924 - val_acc: 0.9476\n",
      "Epoch 386/400\n",
      "1960/1960 [==============================] - 0s 238us/step - loss: 0.0515 - acc: 0.9949 - val_loss: 0.1923 - val_acc: 0.9476\n",
      "Epoch 387/400\n",
      "1960/1960 [==============================] - 1s 295us/step - loss: 0.0491 - acc: 0.9969 - val_loss: 0.1923 - val_acc: 0.9476\n",
      "Epoch 388/400\n",
      "1960/1960 [==============================] - 0s 227us/step - loss: 0.0551 - acc: 0.9929 - val_loss: 0.1923 - val_acc: 0.9476\n",
      "Epoch 389/400\n",
      "1960/1960 [==============================] - 1s 290us/step - loss: 0.0504 - acc: 0.9949 - val_loss: 0.1923 - val_acc: 0.9476\n",
      "Epoch 390/400\n",
      "1960/1960 [==============================] - 0s 227us/step - loss: 0.0520 - acc: 0.9939 - val_loss: 0.1923 - val_acc: 0.9476\n",
      "Epoch 391/400\n",
      "1960/1960 [==============================] - 0s 250us/step - loss: 0.0563 - acc: 0.9954 - val_loss: 0.1923 - val_acc: 0.9476\n",
      "Epoch 392/400\n",
      "1960/1960 [==============================] - 0s 228us/step - loss: 0.0596 - acc: 0.9918 - val_loss: 0.1922 - val_acc: 0.9476\n",
      "Epoch 393/400\n",
      "1960/1960 [==============================] - 1s 267us/step - loss: 0.0518 - acc: 0.9954 - val_loss: 0.1922 - val_acc: 0.9476\n",
      "Epoch 394/400\n",
      "1960/1960 [==============================] - 0s 219us/step - loss: 0.0529 - acc: 0.9969 - val_loss: 0.1922 - val_acc: 0.9476\n",
      "Epoch 395/400\n",
      "1960/1960 [==============================] - 0s 249us/step - loss: 0.0516 - acc: 0.9949 - val_loss: 0.1922 - val_acc: 0.9476\n",
      "Epoch 396/400\n",
      "1960/1960 [==============================] - 0s 226us/step - loss: 0.0485 - acc: 0.9980 - val_loss: 0.1922 - val_acc: 0.9476\n",
      "Epoch 397/400\n",
      "1960/1960 [==============================] - 1s 324us/step - loss: 0.0562 - acc: 0.9939 - val_loss: 0.1922 - val_acc: 0.9476\n",
      "Epoch 398/400\n",
      "1960/1960 [==============================] - 0s 215us/step - loss: 0.0513 - acc: 0.9974 - val_loss: 0.1922 - val_acc: 0.9476\n",
      "Epoch 399/400\n",
      "1960/1960 [==============================] - 0s 254us/step - loss: 0.0551 - acc: 0.9954 - val_loss: 0.1922 - val_acc: 0.9476\n",
      "Epoch 400/400\n",
      "1960/1960 [==============================] - 0s 213us/step - loss: 0.0563 - acc: 0.9959 - val_loss: 0.1922 - val_acc: 0.9476\n"
     ]
    }
   ],
   "source": [
    "# it may takes several times to reach the reported performance\n",
    "import keras\n",
    "lr = 1e-3\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])\n",
    "lrScheduler = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, cooldown=5, min_lr=5e-6)\n",
    "history = DD_Net.fit([X_0,X_1],Y,\n",
    "            batch_size=len(Y),\n",
    "            epochs=400,\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            callbacks=[lrScheduler],\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFnCAYAAABdOssgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XtcVWXe9/Hv5pSBqKCAZdFYBsp4\nTMcZD6UpiYcO5lnTyjQ1PKQzeSA1uMdUUEcsy6lx1NsxE029nabJcNQcZW7jeRRfhpVmmmJCSQJC\nHBTZ+/nDV/uJBDbOyGJf+Hn/tfdisfbvxyV+Wdfa+1o2h8PhEAAAMJJHbRcAAAD+fQQ5AAAGI8gB\nADAYQQ4AgMEIcgAADEaQAwBgMIIccBPh4eGaNm3addtffvllhYeH3/Dx5s6dq5UrV1a5z/bt2/Xs\ns8/e8LEBuA+CHHAjJ06c0A8//OB8fuXKFR07dqwWKwLg7ghywI38+te/1j/+8Q/n85SUFLVp06bc\nPjt37tSjjz6qvn376umnn1ZGRoYkKTc3V88995x69eqlCRMmqKCgwPk9X331lUaPHq2oqCg99thj\nSk9Pd1nLm2++qaioKEVGRmrixInKz8+XJJWUlGjWrFnq1auX+vXrp7/+9a9Vbp8zZ45WrVrlPO5P\nn/fq1UtvvPGGoqKilJmZqdOnT2vkyJHq16+fHnnkEX3wwQfO7ztw4IAGDBigqKgoTZw4UXl5eZo2\nbZrWrFnj3OfEiRP6zW9+o6tXr1bvBw7UAQQ54Eb69etXLrz+/ve/q2/fvs7nmZmZmj9/vt588019\n9NFH6tmzp1555RVJ0urVqxUQEKC9e/fqlVdeUUpKiiTJbrdrxowZeuKJJ5ScnKy4uDhFR0dXGXbH\njh3Txo0btW3bNu3atUtXrlzRO++8I0lau3atSktLtXfvXq1bt06vvvqqvvvuu0q3u/Ldd98pOTlZ\nd955p5YsWaKHH35YO3fu1KJFizR37lyVlpaqqKhIv/vd75SYmKjk5GSFhobqtdde06OPPlru57V7\n92716dNHXl5eN/aDBwxGkANupHPnzjp58qQuXryokpISHTlyRF26dHF+/V//+pd+/etf65577pEk\nDR06VKmpqSotLdWhQ4fUr18/SdJdd92lzp07S5JOnz6tjIwMDR48WJLUsWNHBQYG6siRI5XW0bp1\na+3bt0/169eXh4eHOnTooHPnzkmS9u/frwEDBkiSmjZtqn379ikkJKTS7a707NnT+XjVqlUaN26c\ns87Lly8rOztbaWlpuuOOOxQWFiZJmjlzpmJiYtSjRw9lZGTo9OnTkq4Fef/+/V2+JlCX8Gcr4EY8\nPT3Vp08f7dy5U4GBgerevXu5s8vc3Fw1aNDA+dzf318Oh0N5eXm6dOmS/P39nV/7cb/8/HyVlZWV\nC7gffvhBeXl5ldZRXFysxYsXKzU1VZJ06dIlZ+Dm5uaWex0/P78qt7vSsGFD5+MDBw7oj3/8o3Jz\nc2Wz2eRwOGS326/r28fHx/n4xyn4IUOGKDs72/kHDHCrIMgBN9O/f38lJiYqICBAo0aNKve1xo0b\nlzuTvnTpkjw8PBQQEKAGDRqUuy6ek5Oju+++W8HBwfLz89NHH3103Wtt3769whrWr1+vM2fOaPv2\n7fLz81NiYqJzmjwgIEC5ubnOfb/99ls1bNiw0u0eHh6y2+3O7Xl5eQoNDb3uNUtLSzV9+nStWLFC\nPXr00JUrV9S2bdsKX7O4uFiXLl1S06ZNNWDAAC1evFj+/v6KioqShwcTjbi18C8ecDMdOnTQhQsX\ndPLkyevOLrt166ZDhw45p7mTkpLUrVs3eXl5qX379tq9e7ckKSMjQ4cPH5YkNWvWTE2bNnUGeU5O\njn7729+qqKio0houXryo5s2by8/PT+fPn9e+fftUWFgo6dob1Hbs2CGHw6Hs7GwNHDhQOTk5lW4P\nCgrS8ePHJUnnzp2rdEq/uLhYRUVFioiIkHTtjwlvb28VFhaqY8eOys7O1qeffirp2hT8m2++KUnq\n2rWr8vLytGHDBuelBeBWwhk54GZsNpseeeQRFRcXX3d22bRpUy1YsMD5ZrVmzZppwYIFkqSJEydq\nxowZ6tWrl+677z716dPHebzly5crLi5OK1askIeHh8aOHStfX99KaxgxYoSmTp2qXr16qXXr1oqJ\nidHkyZO1bt06Pfvsszp79qwefvhh1atXT7Nnz1azZs0q3T5s2DBNmTJFffr0UUREhKKioip8zQYN\nGmj8+PF67LHH1LRpU73wwguKjIzU+PHjlZycrJUrV2rmzJmSpHvuuUfx8fGSrl2O6Nu3r3bv3q2O\nHTv+xz9/wDQ27kcOwHSrV69Wbm6uZs2aVdulAJZjah2A0XJycrRlyxaNHDmytksBakWNBvmXX36p\nyMhI5+dPs7KyNGbMGI0aNUovvviirly5Ikl6//33NXjwYA0dOlRbt26tyZIA1CFJSUkaPHiwnn/+\ned199921XQ5QK2psar2oqEgTJ07UL37xC4WHh2v06NGKiYnRQw89pH79+mnJkiW66667NHDgQD35\n5JPaunWrvL29NXDgQCUlJalRo0Y1URYAAHVKjZ2R+/j4aPXq1QoODnZuS01NVe/evSVJvXv31sGD\nB3X06FG1adNG/v7+qlevnjp16qS0tLSaKgsAgDqlxt617uXldd0yicXFxc6FHIKCgpSdna3vv/9e\ngYGBzn2aNGmi7OzsmioLAIA6xdI3u9lsNufjH2f0fz6z73A4yu1XkatXy25+cQAAGMjSz5Hffvvt\nKikpUb169fTdd98pODhYISEh2rdvn3OfCxcuqH379lUeJze38oUs/h1BQf7Kzi5wvaMB6MU90Yt7\nqiu91JU+JHqp6liVsfSMvGvXrkpOTpYk7dq1Sw8++KDatWun9PR05efnq7CwUGlpaerUqZOVZQEA\nYKwaOyM/duyYEhISdP78eXl5eSk5OVnLli3TnDlztHnzZt15550aOHCgvL299bvf/U7jxo2TzWbT\n5MmTy914AQAAVM7Ild1u9rQLUznuiV7cE724n7rSh0QvVR2rMqzsBgCAwQhyAAAMRpADAGAwghwA\nAINxP/KbZOXKRJ048YVyci6qpKREd97ZTA0aNNSiRUur/L4PP/yb/Pzqq0ePhy2qFABQlxDkN8nU\nqTMkXQvm06dPacqU6dX6vv79H6vJsgAAdRxBXoPS0g4pKekdFRUVacqUGTpy5LD27dsju92uLl26\n6bnnJmjNmrfVqFEjNW9+n7Zv3yKbzUNnz36tnj1767nnJtR2CwAAN1cng3zL3q/0f49fqPb+np42\nlZVV/XH6X7UM1rBeLW64llOnvtKmTdvl4+OjI0cOa9WqP8vDw0PDhj2h4cNHldv3888/07vvbpPd\nbtfQoY8R5AAAl+pkkLuTFi3ud97xrV69epoyZYI8PT2Vl5en/Pz8cvuGh7dUvXr1aqNMAICh6mSQ\nD+vV4obOnmtyJSFvb29J0rffZmnz5o1au3ajfH19NWbMsOv29fT0rJEaAAB1Fx8/s0heXp4CAgLk\n6+urEyeO69tvv1VpaWltlwUAMBxBbpH77w/T7bf76oUXntOePbv0xBOD9Ic/JNR2WQAAw3HTFLFI\nv7uiF/dEL+6nrvQh0UtVx6oMZ+QAABiMIAcAwGAEOQAABiPIAQAwGEEOAIDBCHIAAAxWJ1d2qw3/\n7m1Mf5SVlalLl/LUsmVEDVcKAKhLCPKb5N+9jemPDh36Pyoru0qQAwBuCEFew1atel2ffZYuu71M\nQ4aMVO/ej+jgwX9p7dq35eNzm5o0aaLJk6frv//7z/L29lFwcFN17dq9tssGABiiTgb59q8+0JEL\n6dXe39PDpjJ71QvcdQhuo0EtHr2hOtLSDik3N0dvvrlaly+XaNy4p/Xggz20bdtmvfjiS2rduq0+\n/ni3vL29FRXVX8HBwYQ4AOCG1Mkgdxfp6UeVnn5UU6Zcu6+43V6mnJyLevjhSCUkvKo+ffrrkUei\nFBAQWMuVAgBMVSeDfFCLR2/o7Lmm1vb19vbW448/qVGjni63fcCAx9WlSzft379PM2e+qEWLlt30\n1wYA3Br4+FkNiohorX/964DsdrtKSkq0YsW1wF63brV8fG7TwIGD1bNnb509+7U8PDx05Qq3NQUA\n3Jg6eUbuLtq3f0CtW7fVxIljJTk0ePBwSVJQULCmTZskf/8GatiwoUaPfkZeXt5avPj3CgwMVGRk\nVO0WDgAwBrcxFbfNc1f04p7oxf3UlT4keqnqWJVhah0AAIMR5AAAGIwgBwDAYAQ5AAAGI8gBADAY\nQQ4AgMEIcgAADEaQAwBgMIIcAACDEeQAABiMIAcAwGAEOQAABiPIAQAwGEEOAIDBCHIAAAxGkAMA\nYDCCHAAAgxHkAAAYjCAHAMBgBDkAAAYjyAEAMBhBDgCAwQhyAAAM5mXli9ntdsXGxurkyZPy9vZW\nXFyccnJytHz5cnl5ecnX11dLlixRw4YNrSwLAABjWRrke/bsUUFBgZKSkpSRkaGFCxcqOztby5Yt\n07333qu33npLmzdv1oQJE6wsCwAAY1k6tX7mzBm1bdtWkhQaGqrMzEw1bNhQeXl5kqRLly4pICDA\nypIAADCapUEeFhamlJQUlZWV6fTp0zp37pxeeOEFTZ48WVFRUTp8+LCefPJJK0sCAMBoNofD4bDy\nBRMTE5Wamqrw8HClp6erQYMGmjp1qjp27KiEhATdcccdevrpp6s8xtWrZfLy8rSoYgAA3Jel18gl\nacaMGc7HkZGRysrKUseOHSVJXbt21d/+9jeXx8jNLbqpNQUF+Ss7u+CmHrO20It7ohf3VFd6qSt9\nSPRS1bEqY+nU+vHjxxUTEyNJ2r9/vyIiItSkSRN99dVXkqT09HTdc889VpYEAIDRLD0jDwsLk8Ph\n0PDhw+Xv76+EhASdPXtW8+bNk7e3txo2bKhFixZZWRIAAEazNMg9PDwUHx9fblvjxo2VlJRkZRkA\nANQZrOwGAIDBCHIAAAxGkAMAYDCCHAAAgxHkAAAYjCAHAMBgBDkAAAYjyAEAMBhBDgCAwQhyAAAM\nRpADAGAwghwAAIMR5AAAGIwgBwDAYAQ5AAAGI8gBADAYQQ4AgMEIcgAADEaQAwBgMIIcAACDEeQA\nABiMIAcAwGAEOQAABiPIAQAwGEEOAIDBCHIAAAxGkAMAYDCCHAAAgxHkAAAYjCAHAMBgBDkAAAYj\nyAEAMBhBDgCAwQhyAAAMRpADAGAwghwAAIMR5AAAGIwgBwDAYAQ5AAAGI8gBADAYQQ4AgMEIcgAA\nDEaQAwBgMIIcAACDEeQAABiMIAcAwGAEOQAABiPIAQAwGEEOAIDBCHIAAAxGkAMAYDAvK1/Mbrcr\nNjZWJ0+elLe3t+Li4hQaGqo5c+bo7Nmz8vPz0+uvv66GDRtaWRYAAMayNMj37NmjgoICJSUlKSMj\nQwsXLtRDDz2kgIAA/eEPf9DmzZt16NAh9e7d28qyAAAwlqVBfubMGbVt21aSFBoaqszMTH388cea\nNm2aJGn48OFWlgMAgPFsDofDYdWL/fOf/9T69eu1evVqnT17VoMGDdIdd9yhIUOG6H//93/VoEED\nxcbGqlGjRlUe5+rVMnl5eVpUNQAA7svSIJekxMREpaamKjw8XOnp6SoqKtK0adPUv39/rVq1SgUF\nBZo9e3aVx8jOLripNQUF+d/0Y9YWenFP9OKe6kovdaUPiV6qOlZlLJ1al6QZM2Y4H0dGRio4OFid\nOnWSJHXv3l0rV660uiQAAIxl6cfPjh8/rpiYGEnS/v37FRERoZ49e+rAgQOSpM8++0zNmze3siQA\nAIxm6Rl5WFiYHA6Hhg8fLn9/fyUkJMjX11dz587Vjh075OPjo4SEBCtLAgDAaJYGuYeHh+Lj46/b\nvnz5civLAACgzmBlNwAADEaQAwBgMIIcAACDuQzyU6dOWVEHAAD4N7gM8qlTp2rkyJHatm2biouL\nragJAABUk8t3rX/44Yf68ssvtXPnTo0ZM0atWrXS0KFDnWumAwCA2lOta+RhYWF68cUXNWfOHJ06\ndUrR0dF66qmndObMmRouDwAAVMXlGXlmZqa2b9+uDz74QC1atNCkSZP04IMPKj09XTNnztR7771n\nRZ0AAKACLoN89OjRGjJkiNavX6+QkBDn9rZt2zK9DgBALXM5tf7+++/rF7/4hTPEN23apMLCQknS\n/Pnza7Y6AABQJZdBHhMTo2+++cb5vKSkRLNmzarRogAAQPW4DPK8vDxNmDDB+Xzs2LHKz8+v0aIA\nAED1uAzy0tLScovCpKenq7S0tEaLAgAA1ePyzW4xMTGKjo5WQUGBysrKFBgYqCVLllhRGwAAcMFl\nkLdr107JycnKzc2VzWZTo0aNlJaWZkVtAADABZdB/sMPP+ivf/2rcnNzJV2bat+2bZtSUlJqvDgA\nAFA1l9fIp0+frhMnTmj79u0qLCzUxx9/rLi4OAtKAwAArrgM8suXL+v3v/+9mjVrptmzZ+svf/mL\ndu7caUVtAADAhWq9a72oqEh2u125ublq1KiRzp07Z0VtAADABZfXyJ944glt2bJFQ4cOVf/+/eXn\n56ewsDAragMAAC64DPIRI0bIZrNJkrp06aKLFy+qVatWNV4YAABwzeXU+tNPP+18HBISooiICGew\nAwCA2uXyjLxVq1Z67bXX1KFDB3l7ezu3d+nSpUYLAwAArrkM8i+++EKSdOjQIec2m81GkAMA4AZc\nBvmGDRusqAMAAPwbXAb5qFGjKrwmvnHjxhopCAAAVJ/LIJ8+fbrzcWlpqT755BP5+vrWaFEAAKB6\nXAZ5586dyz3v1q2bnn/++RorCAAAVJ/LIP/5Km5ZWVn6+uuva6wgAABQfS6D/JlnnnE+ttlsql+/\nvqZMmVKjRQEAgOpxGeR79+6V3W6Xh8e1tWNKS0vLfZ4cAADUHpcruyUnJ2vSpEnO50899ZQ++uij\nGi0KAABUj8sgX7dunRYvXux8vnbtWq1bt65GiwIAANXjMsgdDocaN27sfF6/fn3WWgcAwE24vEbe\nunVrTZ8+XZ07d5bD4dCBAwfUunVrK2oDAAAuuAzyefPm6f3339enn34qm82mxx9/XH379rWiNgAA\n4ILLIC8uLpa3t7fmz58vSdq0aZOKi4vl5+dX48UBAICqubxGPnv2bH3zzTfO5yUlJZo1a1aNFgUA\nAKrHZZDn5eVpwoQJzudjx45Vfn5+jRYFAACqx2WQl5aW6tSpU87nn376qUpLS2u0KAAAUD0ur5HH\nxMQoOjpaBQUFstvtCggI0JIlS6yoDQAAuOAyyNu1a6fk5GRlZWUpNTVVO3bs0AsvvKCUlBQr6gMA\nAFVwGeRHjx7Vtm3btHPnTpWVlWnBggXq06ePFbUBAAAXKr1G/uc//1n9+/fX9OnTFRgYqG3btik0\nNFQDBgzgpikAALiJSs/IExMT1aJFC73yyiv6zW9+I0kszQoAgJupNMj37dun//mf/1FsbKzsdrue\nfPJJ3q0OAICbqXRqPSgoSBMmTFBycrIWLlyos2fP6vz585o0aZL++c9/WlkjAACohMvPkUtS586d\nlZCQoAMHDqhHjx564403arouAABQDdUK8h/Vr19fI0eO1HvvvVdT9QAAgBtwQ0EOAADcC0EOAIDB\nLA1yu92u+fPna8SIERozZky5NdwPHDig8PBwK8sBAMB4lgb5nj17VFBQoKSkJC1cuNC5Zvvly5f1\npz/9SUFBQVaWAwCA8SwN8jNnzqht27aSpNDQUGVmZqqsrExvvfWWRo0aJR8fHyvLAQDAeC7XWr+Z\nwsLCtH79ej3zzDM6e/aszp07p2PHjun48eN68cUXtXTp0modJyDAV15enje1tqAg/5t6vNpEL+6J\nXtxTXemlrvQh0cuNsjTIe/ToobS0ND311FMKDw/Xvffeq+XLl2vBggU3dJzc3KKbWldQkL+yswtu\n6jFrC724J3pxT3Wll7rSh0QvVR2rMpYGuSTNmDHD+bhXr17Ky8vTSy+9JEm6cOGCRo8erXfeecfq\nsgAAMJKl18iPHz+umJgYSdL+/fvVunVr7d27V1u2bNGWLVsUHBxMiAMAcAMsv0bucDg0fPhw+fv7\nKyEhwcqXBwCgzrE0yD08PBQfH1/p1/fu3WthNQAAmI+V3QAAMBhBDgCAwQhyAAAMRpADAGAwghwA\nAIMR5AAAGIwgBwDAYAQ5AAAGI8gBADAYQQ4AgMEIcgAADEaQAwBgMIIcAACDEeQAABiMIAcAwGAE\nOQAABiPIAQAwGEEOAIDBCHIAAAxGkAMAYDCCHAAAgxHkAAAYjCAHAMBgBDkAAAYjyAEAMBhBDgCA\nwQhyAAAMRpADAGAwghwAAIMR5AAAGIwgBwDAYAQ5AAAGI8gBADAYQQ4AgMEIcgAADEaQAwBgMIIc\nAACDEeQAABiMIAcAwGAEOQAABiPIAQAwGEEOAIDBCHIAAAxGkAMAYDCCHAAAgxHkAAAYjCAHAMBg\nBDkAAAYjyAEAMBhBDgCAwbysfDG73a7Y2FidPHlS3t7eiouLk6+vr2JiYnT16lV5eXlp6dKlCgoK\nsrIsAACMZWmQ79mzRwUFBUpKSlJGRoYWLlyoRo0aadiwYerfv782btyodevWadasWVaWBQCAsSwN\n8jNnzqht27aSpNDQUGVmZioxMVG33XabJCkgIECfffaZlSUBAGA0S6+Rh4WFKSUlRWVlZTp9+rTO\nnTunoqIieXp6qqysTO+++64ee+wxK0sCAMBoNofD4bDyBRMTE5Wamqrw8HClp6fr7bffVmBgoGbN\nmqXmzZtrypQpLo9x9WqZvLw8LagWAAD3ZnmQ/1RkZKR27dqlOXPm6K677tK0adOq9X3Z2QU3tY6g\nIP+bfszaQi/uiV7cU13ppa70IdFLVceqjKVT68ePH1dMTIwkaf/+/YqIiNAHH3wgb2/vaoc4AAD4\n/yx9s1tYWJgcDoeGDx8uf39/JSQkaPLkybp8+bLGjBkjSbrvvvsUFxdnZVkAABjL0iD38PBQfHx8\nuW1JSUlWlgAAQJ3Cym4AABiMIAcAwGAEOQAABiPIAQAwGEEOAIDBCHIAAAxGkAMAYDCCHAAAgxHk\nAAAYjCAHAMBgBDkAAAYjyAEAMBhBDgCAwQhyAAAMRpADAGAwghwAAIMR5AAAGIwgBwDAYAQ5AAAG\nI8gBADAYQQ4AgMEIcgAADEaQAwBgMIIcAACDEeQAABiMIAcAwGAEOQAABiPIAQAwGEEOAIDBCHIA\nAAxGkAMAYDCCHAAAgxHkAAAYjCAHAMBgBDkAAAYjyAEAMBhBDgCAwQhyAAAMRpADAGAwghwAAIMR\n5AAAGIwgBwDAYAQ5AAAGI8gBADAYQQ4AgMEIcgAADEaQAwBgMIIcAACDEeQAABiMIAcAwGAEOQAA\nBvOy8sXsdrtiY2N18uRJeXt7Ky4uTr6+vpo1a5bKysoUFBSkpUuXysfHx8qyAAAwlqVBvmfPHhUU\nFCgpKUkZGRlauHChAgMDNWrUKPXr109LlizR1q1bNWrUKCvLAgDAWJZOrZ85c0Zt27aVJIWGhioz\nM1Opqanq3bu3JKl37946ePCglSUBAGA0S4M8LCxMKSkpKisr0+nTp3Xu3DmdP3/eOZUeFBSk7Oxs\nK0sCAMBolk6t9+jRQ2lpaXrqqacUHh6ue++9V19++aXz6w6Ho1rHCQryv+m11cQxawu9uCd6cU91\npZe60odELzfK0iCXpBkzZjgfR0ZGKiQkRCUlJapXr56+++47BQcHW10SAADGsnRq/fjx44qJiZEk\n7d+/XxEREeratauSk5MlSbt27dKDDz5oZUkAABjN5qjufPZNYLfb9fLLL+vrr7+Wv7+/EhISVFZW\nptmzZ+vy5cu68847tXjxYnl7e1tVEgAARrM0yAEAwM3Fym4AABiMIAcAwGCWv2vd3SxatEhHjx6V\nzWbTyy+/7FywxgTHjh1TdHS07rnnHknXPqc/fvx4o5a8/fLLLxUdHa1nn31Wo0ePVlZWVoX1v//+\n+1q/fr08PDw0fPhwDRkypLZLv87Pe1mwYIGOHDkiPz8/SdK4cePUs2dPI3pZsmSJDh8+rKtXr2ri\nxIlq06aNsePy815SU1ONG5fi4mLNmTNHFy9e1OXLlxUdHa2WLVsaOSYV9ZKSkmLcmPxUSUmJBgwY\noMmTJ6tLly7Wj4vjFpaamuqYMGGCw+FwOE6ePOkYMmRILVd0Y1JTUx2vvvpquW1z5sxxfPjhhw6H\nw+FISEhwbNy4sTZKq5bCwkLH6NGjHfPmzXNs2LDB4XBUXH9hYaGjT58+jvz8fEdxcbEjKirKkZub\nW5ulX6eyXj7//PPr9nP3Xg4ePOgYP368w+FwOHJychw9evQwdlwq68W0cfn73//u+NOf/uRwOByO\nb775xtGnTx9jx6SyXkwbk59avny5Y9CgQY5t27bVyrjc0lPrBw8eVGRkpCSpRYsWys/P1w8//FDL\nVVVfYWHhddtMWvLWx8dHq1evLrd2QEX1Hz16VG3atJG/v7/q1aunTp06KS0trbbKrlBFvVQ0Pib0\n8qtf/UqvvfaaJKlhw4YqLi42dlwq6iU/P/+6/dy9l/79++v555+XJGVlZSkkJMTYMamoF1N/VyTp\n1KlT+uqrr9SzZ09JtfN/2C09tf7999/rl7/8pfN548aNlZ2drfr169diVdVXVFSkw4cPa/z48Sou\nLtbUqVNVXFxszJK3Xl5e8vIq/0+wovq///57BQYGOvdp0qSJ2/VVUS+FhYV64403lJ+fr5CQEM2b\nN8+IXjw9PeXr6ytJeu+99/TQQw8pJSXFyHGpqJecnBwjx0WSRowYoW+//VZvvfWWxo4da+SY/Oin\nvSxdutTYMUlISND8+fO1Y8cOSbXzf9gtHeSOn33yzuFwyGaz1VI1N65ly5aaPHmyevfura+//lpj\nx47V1atXnV//eX8m+OnP/8f6TR2nESNGqEWLFmrevLn++Mc/auXKlWrXrl25fdy5l927d2vr1q1a\nu3atoqKinNtNHJef9vLJJ58YOy5JSUn64osvNHPmTON/V37ay7Rp04wckx07dqh9+/a6++67ndtq\nY1xu6an1kJAQff/9987nFy4v7ZKzAAAEYklEQVRcUJMmTWqxohtz3333OadwmjdvriZNmig/P18l\nJSWSZOSSt7fffvt19Vc0TkFBQbVVYrU98sgjat68ufPxiRMnjOnlwIEDeuutt7R69Wr5+/sbPS4/\n78XEcTl27JiysrIkSa1atVJZWZmxY1JRLw888IBxYyJJ+/bt0549ezRs2DC99957WrVqVa2Myy0d\n5N26dXMuD/v5558rODjYmGl1Sdq6dav+8pe/SJKys7N18eJFDRo0yOglbytasrddu3ZKT09Xfn6+\nCgsLlZaWpk6dOtVypa5NmjRJmZmZkq5dN7v//vuN6KWgoEBLlizR22+/rUaNGkkyd1wq6sXEcTl0\n6JDWrl0r6dolwaKiImPHpKJe5s6da9yYSNKKFSu0bds2bdmyRUOHDlV0dHStjMstv7LbsmXLdOjQ\nIdlsNsXGxqply5a1XVK1Xbp0SS+99JKKiop05coVTZkyRa1atTJmydtjx44pISFB58+fl5eXl0JC\nQrRs2TLNmTPnuvo/+ugjrVmzRjabTaNHj9bjjz9e2+WXU1EvI0eO1Jo1a+Tr66vbb79dixcvVuPG\njd2+l82bN2vlypXOMyRJio+P17x584wbl4p6GTx4sDZs2GDUuJSUlGju3LnKyspSSUmJpkyZotat\nW1f4u+7OfUgV9+Lj46PExESjxuTnVq5cqWbNmql79+6Wj8stH+QAAJjslp5aBwDAdAQ5AAAGI8gB\nADAYQQ4AgMEIcgAADHZLr+wG3Kq++eYb9e3bVx06dCi3vUePHho/fvx/fPzU1FStWLFCmzZt+o+P\nBaBqBDlwiwoMDNSGDRtquwwA/yGCHEA5ERERio6OVmpqqgoLCxUfH6+wsDAdPXpU8fHx8vLyks1m\n0yuvvKIWLVrozJkzmj9/vux2u2677TYtXrxYkmS32xUbG6svvvhCPj4+evvtt533mwZw83CNHEA5\nZWVluv/++7VhwwaNHDlSr7/+uiRp1qxZiomJ0YYNGzR27Fj913/9lyQpNjZW48aN08aNG/Xoo49q\n586dkq7d3nHq1KnasmWLvLy8lJKSUms9AXUZZ+TALSonJ0djxowpt23mzJmSpO7du0uSHnjgAa1Z\ns0b5+fm6ePGi2rZtK0nq3Lmzfvvb30qSPv30U3Xu3FmSNGjQIEnXrpHfe++9zpsQNW3atML7gAP4\nzxHkwC2qqmvkP1252WazXXfLxZ+v7Gy32687hqen502oEoArTK0DuM4nn3wiSTp8+LDCw8Pl7++v\noKAgHT16VJJ08OBBtW/fXtK1s/YDBw5Ikj788EMtX768dooGblGckQO3qIqm1u+66y5J127ru2nT\nJl26dEkJCQmSpISEBMXHx8vT01MeHh6Ki4uTJM2fP1/z58/Xu+++Ky8vLy1atEgZGRmW9gLcyrj7\nGYBywsPD9dlnn8nLi7/zARMwtQ4AgME4IwcAwGCckQMAYDCCHAAAgxHkAAAYjCAHAMBgBDkAAAYj\nyAEAMNj/A+WQsgcjnSmLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.ylim(90,100)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DD_Net.save_weights('weights/coarse_heavy.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with frame_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 5e-6\n",
    "DD_Net.compile(loss=\"categorical_crossentropy\",optimizer=adam(lr),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for e in range(epochs):\n",
    "    print('epoch{}'.format(e))\n",
    "    X_0 = []\n",
    "    X_1 = []\n",
    "    Y = []\n",
    "    \n",
    "    for i in tqdm(range(len(Train['pose']))): \n",
    "    \n",
    "        label = np.zeros(C.clc_coarse)\n",
    "        label[Train['coarse_label'][i]-1] = 1 \n",
    "        \n",
    "        p = np.copy(Train['pose'][i]).reshape([-1,22,3])\n",
    "        p = sampling_frame(p,C)\n",
    "       \n",
    "        p = normlize_range(p)\n",
    "        M = get_CG(p,C)\n",
    "        \n",
    "        X_0.append(M)\n",
    "        X_1.append(p)\n",
    "        Y.append(label)\n",
    "\n",
    "    X_0 = np.stack(X_0)  \n",
    "    X_1 = np.stack(X_1) \n",
    "    Y = np.stack(Y)\n",
    "   \n",
    "\n",
    "    DD_Net.fit([X_0,X_1],Y,\n",
    "            batch_size=len(Y),\n",
    "            epochs=1,\n",
    "            verbose=True,\n",
    "            shuffle=True,\n",
    "            validation_data=([X_test_0,X_test_1],Y_test)      \n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = DD_Net.predict([X_test_0,X_test_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['Grab', 'Tap', 'Expand', 'Pinch', 'RC', 'RCC', 'SR', 'SL', 'SU', 'SD', 'SX', 'S+', 'SV', 'Shake']\n",
    "\n",
    "y_true = []\n",
    "for i in np.argmax(Y_test,axis=1):\n",
    "    y_true.append(labels[i])\n",
    "    \n",
    "y_pred = []\n",
    "for i in np.argmax(Y_pred,axis=1):\n",
    "    y_pred.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm_analysis(y_true,y_pred, 'images/SHREC_14.png', labels, ymap=None, figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
